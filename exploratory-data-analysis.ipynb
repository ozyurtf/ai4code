{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae4226a",
   "metadata": {},
   "source": [
    "### Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b9e09a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googletrans==4.0.0rc1 in /opt/anaconda3/lib/python3.9/site-packages (4.0.0rc1)\n",
      "Requirement already satisfied: httpx==0.13.3 in /opt/anaconda3/lib/python3.9/site-packages (from googletrans==4.0.0rc1) (0.13.3)\n",
      "Requirement already satisfied: idna==2.* in /opt/anaconda3/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (2.10)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (2021.10.8)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (1.2.0)\n",
      "Requirement already satisfied: chardet==3.* in /opt/anaconda3/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (3.0.4)\n",
      "Requirement already satisfied: httpcore==0.9.* in /opt/anaconda3/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (0.9.1)\n",
      "Requirement already satisfied: hstspreload in /opt/anaconda3/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (2022.11.1)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /opt/anaconda3/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (1.5.0)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /opt/anaconda3/lib/python3.9/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in /opt/anaconda3/lib/python3.9/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in /opt/anaconda3/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install googletrans==4.0.0rc1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56e6329",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9728d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using state Massachusetts server backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import ast\n",
    "import kaleido\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import re\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "import concurrent.futures\n",
    "from utils import translate\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbbf383",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46590740",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = pd.read_csv('train_final.csv')\n",
    "train_final['cell_types_shuffled'] = train_final['cell_types_shuffled'].apply(ast.literal_eval)\n",
    "train_final['code_markdowns_shuffled'] = train_final['code_markdowns_shuffled'].apply(ast.literal_eval)\n",
    "\n",
    "test_final = pd.read_csv('test_final.csv')\n",
    "test_final['cell_types_shuffled'] = test_final['cell_types_shuffled'].apply(ast.literal_eval)\n",
    "test_final['code_markdowns_shuffled'] = test_final['code_markdowns_shuffled'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "433240b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_sample = train_final.sample(1000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8b56e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_exploded = train_final.explode(['cell_types_shuffled',\n",
    "                                                   'code_markdowns_shuffled']).reset_index(drop=True)\n",
    "\n",
    "train_final_exploded['code_markdowns_shuffled_translated'] = train_final_exploded['code_markdowns_shuffled'].copy()\n",
    "\n",
    "nonenglish_markdowns = list(train_final_exploded.query('cell_types_shuffled == \"markdown\" & markdown_language != \"en\"')['code_markdowns_shuffled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "980e7ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_types_shuffled</th>\n",
       "      <th>code_markdowns_shuffled</th>\n",
       "      <th>cell_shuffled</th>\n",
       "      <th>cell_order</th>\n",
       "      <th>markdown_language</th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>code_markdowns_shuffled_translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>code</td>\n",
       "      <td># This Python 3 environment comes with many he...</td>\n",
       "      <td>1862f0a6 2a9e43d6 038b763d 2eefe0ef 0beab1cd 9...</td>\n",
       "      <td>1862f0a6 448eb224 2a9e43d6 7e2f170a 038b763d 7...</td>\n",
       "      <td>ru</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "      <td># This Python 3 environment comes with many he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>code</td>\n",
       "      <td>import numpy as np\\nimport pandas as pd\\nimpor...</td>\n",
       "      <td>1862f0a6 2a9e43d6 038b763d 2eefe0ef 0beab1cd 9...</td>\n",
       "      <td>1862f0a6 448eb224 2a9e43d6 7e2f170a 038b763d 7...</td>\n",
       "      <td>ru</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>import numpy as np\\nimport pandas as pd\\nimpor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>code</td>\n",
       "      <td>import warnings\\nwarnings.filterwarnings('igno...</td>\n",
       "      <td>1862f0a6 2a9e43d6 038b763d 2eefe0ef 0beab1cd 9...</td>\n",
       "      <td>1862f0a6 448eb224 2a9e43d6 7e2f170a 038b763d 7...</td>\n",
       "      <td>ru</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>import warnings\\nwarnings.filterwarnings('igno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>code</td>\n",
       "      <td>matplotlib.rcParams.update({'font.size': 14})</td>\n",
       "      <td>1862f0a6 2a9e43d6 038b763d 2eefe0ef 0beab1cd 9...</td>\n",
       "      <td>1862f0a6 448eb224 2a9e43d6 7e2f170a 038b763d 7...</td>\n",
       "      <td>ru</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matplotlib.rcParams.update({'font.size': 14})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>code</td>\n",
       "      <td>def evaluate_preds(train_true_values, train_pr...</td>\n",
       "      <td>1862f0a6 2a9e43d6 038b763d 2eefe0ef 0beab1cd 9...</td>\n",
       "      <td>1862f0a6 448eb224 2a9e43d6 7e2f170a 038b763d 7...</td>\n",
       "      <td>ru</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>def evaluate_preds(train_true_values, train_pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370507</th>\n",
       "      <td>fffc3b44869198</td>\n",
       "      <td>code</td>\n",
       "      <td>test['bookID']</td>\n",
       "      <td>978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...</td>\n",
       "      <td>978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...</td>\n",
       "      <td>et</td>\n",
       "      <td>a6aaa8d7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test['bookID']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370508</th>\n",
       "      <td>fffc3b44869198</td>\n",
       "      <td>code</td>\n",
       "      <td>df = pd.DataFrame(np.nan, index=[0,1,2,3], col...</td>\n",
       "      <td>978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...</td>\n",
       "      <td>978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...</td>\n",
       "      <td>et</td>\n",
       "      <td>a6aaa8d7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>df = pd.DataFrame(np.nan, index=[0,1,2,3], col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370509</th>\n",
       "      <td>fffc3b44869198</td>\n",
       "      <td>code</td>\n",
       "      <td>df</td>\n",
       "      <td>978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...</td>\n",
       "      <td>978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...</td>\n",
       "      <td>et</td>\n",
       "      <td>a6aaa8d7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370510</th>\n",
       "      <td>fffc3b44869198</td>\n",
       "      <td>code</td>\n",
       "      <td>df.to_csv('file_name.csv', index=False)</td>\n",
       "      <td>978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...</td>\n",
       "      <td>978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...</td>\n",
       "      <td>et</td>\n",
       "      <td>a6aaa8d7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>df.to_csv('file_name.csv', index=False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370511</th>\n",
       "      <td>fffc3b44869198</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Lets make test dataset looks like train dataset</td>\n",
       "      <td>978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...</td>\n",
       "      <td>978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...</td>\n",
       "      <td>et</td>\n",
       "      <td>a6aaa8d7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lets make test dataset looks like train dataset</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>548197 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id cell_types_shuffled  \\\n",
       "0        00001756c60be8                code   \n",
       "1        00001756c60be8                code   \n",
       "2        00001756c60be8                code   \n",
       "3        00001756c60be8                code   \n",
       "4        00001756c60be8                code   \n",
       "...                 ...                 ...   \n",
       "6370507  fffc3b44869198                code   \n",
       "6370508  fffc3b44869198                code   \n",
       "6370509  fffc3b44869198                code   \n",
       "6370510  fffc3b44869198                code   \n",
       "6370511  fffc3b44869198            markdown   \n",
       "\n",
       "                                   code_markdowns_shuffled  \\\n",
       "0        # This Python 3 environment comes with many he...   \n",
       "1        import numpy as np\\nimport pandas as pd\\nimpor...   \n",
       "2        import warnings\\nwarnings.filterwarnings('igno...   \n",
       "3            matplotlib.rcParams.update({'font.size': 14})   \n",
       "4        def evaluate_preds(train_true_values, train_pr...   \n",
       "...                                                    ...   \n",
       "6370507                                     test['bookID']   \n",
       "6370508  df = pd.DataFrame(np.nan, index=[0,1,2,3], col...   \n",
       "6370509                                                 df   \n",
       "6370510            df.to_csv('file_name.csv', index=False)   \n",
       "6370511    Lets make test dataset looks like train dataset   \n",
       "\n",
       "                                             cell_shuffled  \\\n",
       "0        1862f0a6 2a9e43d6 038b763d 2eefe0ef 0beab1cd 9...   \n",
       "1        1862f0a6 2a9e43d6 038b763d 2eefe0ef 0beab1cd 9...   \n",
       "2        1862f0a6 2a9e43d6 038b763d 2eefe0ef 0beab1cd 9...   \n",
       "3        1862f0a6 2a9e43d6 038b763d 2eefe0ef 0beab1cd 9...   \n",
       "4        1862f0a6 2a9e43d6 038b763d 2eefe0ef 0beab1cd 9...   \n",
       "...                                                    ...   \n",
       "6370507  978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...   \n",
       "6370508  978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...   \n",
       "6370509  978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...   \n",
       "6370510  978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...   \n",
       "6370511  978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...   \n",
       "\n",
       "                                                cell_order markdown_language  \\\n",
       "0        1862f0a6 448eb224 2a9e43d6 7e2f170a 038b763d 7...                ru   \n",
       "1        1862f0a6 448eb224 2a9e43d6 7e2f170a 038b763d 7...                ru   \n",
       "2        1862f0a6 448eb224 2a9e43d6 7e2f170a 038b763d 7...                ru   \n",
       "3        1862f0a6 448eb224 2a9e43d6 7e2f170a 038b763d 7...                ru   \n",
       "4        1862f0a6 448eb224 2a9e43d6 7e2f170a 038b763d 7...                ru   \n",
       "...                                                    ...               ...   \n",
       "6370507  978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...                et   \n",
       "6370508  978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...                et   \n",
       "6370509  978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...                et   \n",
       "6370510  978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...                et   \n",
       "6370511  978a5137 faa48f03 28dfb12a eea2e812 64fef97c 4...                et   \n",
       "\n",
       "        ancestor_id parent_id  \\\n",
       "0          945aea18       NaN   \n",
       "1          945aea18       NaN   \n",
       "2          945aea18       NaN   \n",
       "3          945aea18       NaN   \n",
       "4          945aea18       NaN   \n",
       "...             ...       ...   \n",
       "6370507    a6aaa8d7       NaN   \n",
       "6370508    a6aaa8d7       NaN   \n",
       "6370509    a6aaa8d7       NaN   \n",
       "6370510    a6aaa8d7       NaN   \n",
       "6370511    a6aaa8d7       NaN   \n",
       "\n",
       "                        code_markdowns_shuffled_translated  \n",
       "0        # This Python 3 environment comes with many he...  \n",
       "1        import numpy as np\\nimport pandas as pd\\nimpor...  \n",
       "2        import warnings\\nwarnings.filterwarnings('igno...  \n",
       "3            matplotlib.rcParams.update({'font.size': 14})  \n",
       "4        def evaluate_preds(train_true_values, train_pr...  \n",
       "...                                                    ...  \n",
       "6370507                                     test['bookID']  \n",
       "6370508  df = pd.DataFrame(np.nan, index=[0,1,2,3], col...  \n",
       "6370509                                                 df  \n",
       "6370510            df.to_csv('file_name.csv', index=False)  \n",
       "6370511    Lets make test dataset looks like train dataset  \n",
       "\n",
       "[548197 rows x 9 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final_exploded.query('markdown_language != \"en\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "93c803a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploratory and data analysis:\n",
      " Form analysis:\n",
      " Identification of the Target: The Target is already identified. It is made up of two values ​​0 for non -presence and 1 for risk. The Target is almost balanced with 46 non -risk of heart stop and 54 risk of heart stop\n",
      " Number of lines and columns: We have 14 variables in our dataset and 303 patients diagnosed. The Target is the 14th variable\n",
      " Types of variables: Dataset is already treated for categorical variables (encoding of category variables). Category variables are all the same identifiable, these are variables: sex, CP, FBS, remaining, exang, slope, ca, thal. The other variables are discreet: AGE, Trestbps, Chol, Thalach and a continuous variable: Oldpeak\n",
      " Identification of missing values: no missing data\n",
      "\n",
      " Basic analysis:\n",
      " Target relationship and continuous quantitative variables: Oldpeak variable (st depression at the level of the electrocar\n",
      "CPU times: user 32 ms, sys: 3.25 ms, total: 35.3 ms\n",
      "Wall time: 660 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(translator.translate(sample_nonenglish_markdown[:1000], dest='en').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5946f138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using state Massachusetts server backend.\n",
      "Using state Massachusetts server backend.\n",
      "Using state Massachusetts server backend.\n",
      "Using state Massachusetts server backend.\n",
      "Using state Massachusetts server backend.\n",
      "Using state Massachusetts server backend.\n",
      "Using state Massachusetts server backend.\n",
      "Using state Massachusetts server backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.4 ms, sys: 243 ms, total: 303 ms\n",
      "Wall time: 3min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == '__main__':\n",
    "    with Pool(8) as p:\n",
    "        translated_markdowns = p.map(translate, nonenglish_markdowns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "15bb6158",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_exploded.loc[train_final_exploded.query('cell_types_shuffled == \"markdown\" & markdown_language != \"en\"').index,\n",
    "                         'code_markdowns_shuffled_translated'] = translated_markdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "34b450da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_types_shuffled</th>\n",
       "      <th>code_markdowns_shuffled</th>\n",
       "      <th>cell_shuffled</th>\n",
       "      <th>cell_order</th>\n",
       "      <th>markdown_language</th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>code_markdowns_shuffled_translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>30d7c3aba9dab5</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Определяем константы\\n\\n\\n\\n Локус варианты в...</td>\n",
       "      <td>dff6ca6f 3fe8e7af 6e32d24f b013be74 ce20afa6 4...</td>\n",
       "      <td>dff6ca6f 3fe8e7af 6e32d24f b013be74 1b01e864 c...</td>\n",
       "      <td>ru</td>\n",
       "      <td>9b72661b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We determine the constants \\n\\n\\n\\n  Locus Opt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id cell_types_shuffled  \\\n",
       "1718  30d7c3aba9dab5            markdown   \n",
       "\n",
       "                                code_markdowns_shuffled  \\\n",
       "1718   Определяем константы\\n\\n\\n\\n Локус варианты в...   \n",
       "\n",
       "                                          cell_shuffled  \\\n",
       "1718  dff6ca6f 3fe8e7af 6e32d24f b013be74 ce20afa6 4...   \n",
       "\n",
       "                                             cell_order markdown_language  \\\n",
       "1718  dff6ca6f 3fe8e7af 6e32d24f b013be74 1b01e864 c...                ru   \n",
       "\n",
       "     ancestor_id parent_id                 code_markdowns_shuffled_translated  \n",
       "1718    9b72661b       NaN  We determine the constants \\n\\n\\n\\n  Locus Opt...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final_exploded.query('cell_types_shuffled == \"markdown\" & markdown_language != \"en\"').sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6412ea4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dataloader creation',\n",
       " 'GCGAN implementation \\n  I want to implement sagan, but it seems that Self Attention is implemented, so gcgan once \\n  Pytorch Book P246 Basically get it as it is \\n  However, it is a color image, so be careful',\n",
       " 'study',\n",
       " 'https: www.kaggle.com crawford cat dataset',\n",
       " 'The pixels are rough, and there is a sign that seems to be ...',\n",
       " 'b: w: h = 1: 3: 64: 64, so OK',\n",
       " 'Generator',\n",
       " 'Discriminator',\n",
       " 'Since the image size is different, it is necessary to cross. \\n  64:64 I wonder if I should make RESIZE and CENTER CROP',\n",
       " 'Conclusion \\n We found that the image had a very high resolution, so we later applied the resize to improve the running time.',\n",
       " '2.3 Augumentation \\n Realize we have quite a few data, ...',\n",
       " 'We have plotted a few images in the training data above (RGB values can be seen by hovering through images). The green parts of the image are of very low blue value, but on the contrary, the brown parts have high blue value. This shows that the green (Healthy) parts of the image are of low blue value, while the urhealthy parts are more likely to have high blue value. This can show that blue channel can be the key to detecting disease in crops',\n",
       " \"Based on the chart, we can choose Adam's learning rate = 10 3.\",\n",
       " \"RGB Analysis \\n Histogram is a graphic representation that indicates the frequency of appearance of different color values in the image. In RGB color space, pixels values range from 0 to 255 where 0 is black and 255 is white. The chart analysis can help us understand the distribution of brightness, contrast and intensity of the image. Now let's see the chart of a random sample from each category.\",\n",
       " '4.2 Save model',\n",
       " 'Observe: \\n The value of the green channel has a more evenly distributed than the red but right channel value, with a smaller vertex. The distribution also has a right deviation (in contrast to the red color) and the mode is greater than 160. This shows that the green color is more clear in these images compared to the red color, this is intentional. Meaning, because this is the image of the leaves',\n",
       " '3.2 Define new model',\n",
       " '2.1 Label encryption \\n Encrypting the labels in the data set of integer format so that the model can be understood.',\n",
       " '5.3 Make submission',\n",
       " 'Distribution of red canals',\n",
       " '4. Model training and evaluation',\n",
       " '5. Make prediction',\n",
       " 'Green canal distribution',\n",
       " 'Observe : \\n The red channel values seem to be almost standard distribution, but slightly deviated to the left (negative deviation). This shows that the red channel tends to focus more at higher values, around 100. There is a major change in the average red value on images.',\n",
       " '3. Define model',\n",
       " 'Observe: \\n The blue channel has a uniform distribution of the three colored channels, with a minimum deviation (slightly deviation to the left). The blue channel shows a major change between the images in the data set.',\n",
       " 'Content \\n\\n  [EDA] (1) \\n  [Data preparation, charting] (1.1) \\n  [Some photos from data set] (1.2) \\n  [RGB Analysis] (1.3) \\n  [Parallel Categories Plot] (1.3)',\n",
       " 'Green canal distribution',\n",
       " 'There are 12 different labels.',\n",
       " 'Learning rate finder\\nhttps: towardsdatascience.com the learning rate finder 6618dfcb2025',\n",
       " '2. Data preparation',\n",
       " 'Summary of channels',\n",
       " '4.1 Model charts \\n  1. Loss \\n  2. Accuracy \\n  3. F1',\n",
       " '1. Set the value of fixed parameters \\n Set the value of fixed parameters in the article: \\n 1. Num Classes: Total number of labels. \\n 2. IMG size: The size of the image after the resized process by dataloader. \\n 3. Batch size: size each batch. \\n 4. Device: Accelerator is used. \\n 5. Criterion: loss function is used.',\n",
       " '4.2 Train model',\n",
       " '2.4 Create custom Dataset',\n",
       " 'Some photos from data set \\n We will check the size of the first 300 images \\n\\n As you can see below, all images are different sizes.',\n",
       " '2.5 Create Dataloader',\n",
       " 'Conclusion \\n\\n  The data set is quite unbalanced according to the circle chart above \\n  We will choose the appropriate sampling strategy to solve this problem. Augmentation data is used to add additional samples from minority layers. In our image, this will be processed by adding distortion to the data by performing, rotating, changing the ratio as well as by adding noise (applying albumentation)',\n",
       " '5.2 Create predictor',\n",
       " '2.2 Create paths for photos and division of training and evaluation',\n",
       " '3.1 Load model if exists',\n",
       " 'Conclusion \\n Analysis of color channels, the blue channel is mainly distributed in the region with strong intensity (because it is concentrated at the end of the graph) == easy to understand because of the full leaf image. For example, the whole leaf image that the red channel is stronger will detect abnormalities. Here the red channel is distributed in the middle area stronger than the blue canal, proving that red it also appears because some leaves are worn or red.',\n",
       " 'Analysis script \\n\\n The following items will be checked: \\n 1. Check the need to adjust the data type \\n 2. Check missing data \\n 3. Evaluate outliers \\n 4. Distribution of selected attributes \\n 5. Relations between selected attributes',\n",
       " '1. Check the need to adjust the data type',\n",
       " '4.3. Neighborhood',\n",
       " '4.8. Daily in local currency',\n",
       " '4.5. Guest capacity',\n",
       " '4. Distribution of selected attributes \\n\\n  4.1.HOST Response Time',\n",
       " '4.7. Number of beds',\n",
       " '4.4. Room type',\n",
       " '4.10. Minimum and maximum night numbers',\n",
       " '4.2. Super host',\n",
       " 'Exploratory analysis of Airbnb data for Prague',\n",
       " '3. Evaluate outliers \\n\\n To evaluate outliers, it is necessary to first identify the columns with numerical data.',\n",
       " '2. Check missing data',\n",
       " '4.6. Number of bedrooms',\n",
       " '4.9. Reviews',\n",
       " '5. Relations between selected attributes',\n",
       " '5.Set',\n",
       " '1.String',\n",
       " 'Make each example for: \\n 1. Strring \\n 2. Tuple \\n 3.list \\n 4. Dictionaries \\n 5.set \\n\\n By using the Python programming language in using the Kaagle platform',\n",
       " '3.List',\n",
       " '2.tupel',\n",
       " '4.dictionaries',\n",
       " 'https: www.kaggle.com yangsaewon ka kr sillim pytorch baseline updated 7 01 \\n  Following the above kernel, I wrote the Inference kernel. Keras can be applied equally.',\n",
       " 'If you have helped, please upvote.',\n",
       " 'We obtain a better precision of 88.5',\n",
       " 'Reconnaissance LSTM',\n",
       " 'There are many shoes',\n",
       " 'We are looking for shoes',\n",
       " 'We obtain an accuracy of 87',\n",
       " 'We will try to find similar clothes',\n",
       " 'There are similar clothes as well as the model',\n",
       " 'Similarity',\n",
       " 'Reconnaissance Conv1D',\n",
       " 'I. Importation des packages',\n",
       " 'The best way to assess the performance of a store type is to see what are the sales by customer in order to normalize everything and get the store that makes its customers spend the most on average. \\n\\n  Let us first compare the total sales of each type of store, its average sales and then see how it changes when we add customers to the equation:',\n",
       " 'We note a spectacular change when we compare having a promo promotion = 1 in the absence of promo promotion = 0 and that we can conclude that a store that has a given day promoting its turnover considerably changes its figure .\\n\\n But, surprisingly, when we more granitate the promo variable2 (indicating a continuous promotion in blue or orange), we find that in general, when there is no consecutive promotion, stores tend to Sell \\u200b\\u200bmore than with a consecutive promotion. This is probably a solution that they are implementing to treat stores whose sales are very low at the start. And indeed, by checking sales by customer compared to promotion, we understand that these stores initially suffer from low sales and that continuous promotion shows a trembling increase in customer purchasing power.\\n\\n If we look at the years, we see a slight increase from one year to the next, but no major change between 2013 and 2015, and we actually observe a very similar scheme over the months, with important peaks First around the Easter period in March and April, then in summer in May, June and July and finally around the Christmas period in November and December.',\n",
       " 'Question 23: \\n  Use https: pandas.pydata.org pandas docs stable reference api pandas.dataframe.corr.html to create the heatmap from the matrix of correlations \\n  Analyzes the highest correlations',\n",
       " '1) CompetitionDistance :',\n",
       " 'Question 15: \\n  Replace the missing values of promo2sinceweek, promo2sinceyear promointerval by zeros \\n  Show the missing DF Store values to verify that everything is 100',\n",
       " 'Question 13: \\n  Replace the missing values with the median',\n",
       " 'As we need digital variables both for our correlation analysis and to supply models based on decision trees, we must transform what is not digital into a digital representation.',\n",
       " 'Feature Engineering',\n",
       " 'Variable Competition Distance',\n",
       " 'We still have storetype, assortment and stateholiday as obejcts we must convert them into digital categories: but we must first make sure that we have not nan before doing these transformations, otherwise nan will be equal to 1',\n",
       " 'Question 30: \\n  Use randomizedsearchcv. Fill the scoring, n jobs, verbose, cv (3), estimator and param distributions parameter \\n  Fitter the model on x train transformed with our variable selection algorithm',\n",
       " 'This case is quite simple, all the missing values come from fields where promo2 = 0 which means that there are no continuous promotional activities for these stores. \\n  The lack of promotion means that these fields must also be 0 since they are linked to promo2.',\n",
       " 'Question 27: \\n  Use a Random Forest with the ESTIMATORS Settings = 10, N JOBS = 1',\n",
       " 'Question 22: \\n  Cut the Competitionistance variable which is continuous in 5 category COMPETITIONDIST CAT',\n",
       " 'Before deciding how to deal with this question, we know that there are infinity of means to fill the missing values. \\n  The most common and simplistic approach is to fill it with the average or the median of this variable. \\n  Let us quickly examine these measures.',\n",
       " \"The Database Train.CSV contains a date. During import with Pandas, this date is recognized as an object. In order to be able to use it optimally and to be able to make feature engineering on this one, we will specify directly in the importion that the column 'date' is a date in Yyyy MM JJ format. For this we will create a function allowing the date directly when importing the file.\",\n",
       " '[](https: encrypted tbn0.gstatic.com images?q=tbn 3AANd9GcQA1NImk thpL2U9 rwsOf SAQw55Ed3HvaOy7J1sdGjvpdq7Lx)',\n",
       " 'Define metric \\n It is difficult to measure the quality of a given model without quantifying its performance in relation to training and tests. This is generally done using a certain type of performance measurement. For this project, we will use the RMSE (Root Mean Square Error) score) Middle quadratic error provided as an evaluation measure for the competition. If the result is less than 10, this means that the prediction is of very good quality and that it is our objective in this project.',\n",
       " 'Question 34: \\n  Rénsrainer XGB with the new parameters \\n  If certain parameters have reached SPI or Max Rangeomizedsearchcv terminals by adapting the parameters',\n",
       " 'IV. Correlation analysis',\n",
       " 'II. Importing datasets',\n",
       " 'Question 7: Show basic DF train statistics on the dirty variables what do you notice?',\n",
       " 'Question 16: \\n  Merfing the two data games on the key store variable',\n",
       " 'Modelization \\n In this section of the project, we will develop a training and an RF adjustment using Gridsearch for the optimization of hyperparameters in order to make a prediction. \\n Then, we will do precise performance assessments of each model (Randomforegressor vs xgboostregressor) using the Sklearn library to help us assess which model is the most suitable.',\n",
       " 'III.2 DF Store Analysis \\n  Question 13: \\n  Show missing value percentage for each variable',\n",
       " 'The promo2sinceweek, Promo2Sinceyear and Promointerval variables have a filling rate of 51 because they are in fact null values, because there is no continuous promotion for these stores. \\n\\n  For the Competitionopensincemonth and Competitionopensinceyear variables, it is essentially missing data that we process here (filling rate of 68.25), which means that we have the distance closest to the competitor but that we miss the information on the date to Which he really opened next to the Rossman store.',\n",
       " 'Question 17: \\n  Calculate the new dirty variable variable corresponding to the number of sales standardized by customers',\n",
       " 'Question 4: \\n  Show the number of days where you have a closed store \\n  Show the number of days where you have a closed store on school holidays but not a public vacation \\n  Show the number of days where you have a closed store coinciding with a public holidays \\n  Display the number of days where you have a closed store coinciding with days or there was no vacation (school or public)',\n",
       " 'We can clearly see here that most stores have either a type of assortment A or a type of assortment C. \\n  It is interesting to note that the type D store, which has the highest customer sales by customer, has in fact a higher type of Assortment C than the others, which most likely explains this high sales by sales by customer. \\n  Another important factor is the fact that the type B store is the only one to have the type of assortment B and that a large number of them are in fact extra and, as figures 1.4 and 1.5 show, show. He is the one who has the greatest number of customers and sales. This extra formula is probably the happy medium for customers between an assortment not too varied such as assortment C and an assortment not too basic like assortment A and this explains high traffic in this store.',\n",
       " 'Question 1: \\n  Import train.csv and store it in DF train, specify the name of the column containing the date and add the function with the date parameter parser \\n  Import blind.csv and store it in DF Store',\n",
       " 'Question 9: \\n  Show boxplot and distribution for the prediction variable',\n",
       " 'Question 8: \\n  Remove lines with open stores and zero sales',\n",
       " '2) CompetitionOpenSinceMonth and CompetitionOpenSinceYear ?',\n",
       " 'Question 19: \\n  In your opinion by studying the distribution of Assortment range and sales, the average basket and the number of customers per type of store that can we deduce? \\n  In general, what do you find the most in terms of range in stores? \\n  Highlight the type of range allowing you to have an elevated basket \\n  Highlight the type of range allowing you to have a high number of customers \\n  Highlight the type of range allowing to have a high number of sales',\n",
       " 'Machine Learning',\n",
       " 'Since we have no information on these missing values and in no precise way to fill them. \\n  A creative means could be to apply a multilabel classification algorithm and train on non -nan fields, then predict what could most likely be the month and year for these fields. But this approach is too long in terms of calculations. \\n  This is why these fields will be assigned to 0.',\n",
       " 'Question 21: \\n  Analyze the promo variable per day \\n  What information can we get from it?',\n",
       " 'Id an Id that represents a (Store, Date) duple within the test set\\n Store a unique Id for each store\\n Sales the turnover for any given day (this is what you are predicting)\\n Customers the number of customers on a given day\\n Open an indicator for whether the store was open: 0 = closed, 1 = open\\n StateHoliday indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\\n SchoolHoliday indicates if the (Store, Date) was affected by the closure of public schools\\n StoreType differentiates between 4 different store models: a, b, c, d\\n Assortment describes an assortment level: a = basic, b = extra, c = extended\\n CompetitionDistance distance in meters to the nearest competitor store\\n CompetitionOpenSince[Month Year] gives the approximate year and month of the time the nearest competitor was opened\\n Promo indicates whether a store is running a promo on that day\\n Promo2 Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\\n Promo2Since[Year Week] describes the year and calendar week when the store started participating in Promo2\\n PromoInterval describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. Feb,May,Aug,Nov means each round starts in February, May, August, November of any given year for that store',\n",
       " 'Sales prediction \\n\\n Rossmann manages more than 3,000 pharmacies in 7 European countries. Currently, the directors of Rossmann stores are responsible for predicting their daily sales up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and national holidays, seasonality and locality. As thousands of managers predict sales according to their particular situation, the accuracy of the results can be very variable. \\n\\n\\n  Goals: \\n\\n  Exploration of data (outliers, Missing values etc.). \\n  Analysis of correlation. \\n  Model \\n  Evaluate the model \\n  Choose the best model',\n",
       " 'III.1 Analysis the DF Train',\n",
       " 'Question 24: \\n  Use Get Dummies on the Assortment variables, Storetype, Promointerval',\n",
       " 'We had to check certain exceptions (aberrant values) in the boxplot to see if the data entered was wrong, but it turns out that this large number of sales on certain days is explained by promotional objectives, the type of store being Large and popular, by the fact that it does not have enough competition and that it is the monopoly in its region. \\n\\n  An important measure to always check when a distribution is the comparison between the average and the median and the distance that separates them. As we see here, an average of 6955 against 6369 in the median is a very good sign that there are no extravagant values affecting the general distribution of sales.',\n",
       " 'III.3 Cross analysis DF Train DF Store',\n",
       " 'Question 10: Display the percentage of higher values strict to 14,000 in your opinion How to explain sales greater than 14K? Should we delete them?',\n",
       " \"Our 5 most important variables are:\\n\\n 1 distance from the competitor: this has a significant impact on sales of a store as we have seen previously in our EDA, while competition is far away the stores tend to sell much more.\\n \\n 2 Promotion: promotion is essential for a store to increase sales, it allows prices to break and therefore interest more customers in purchase.\\n \\n 3 store: The store itself represents a unique identifier for the algorithm allowing to recognize which store which attributes and to better take into account the forecasts of these same stores in the near future.\\n \\n 4 Competitionopensince: The fusion of this variable paid and allowed us to give more precise forecasts of sales according to the time of opening these competitors.\\n \\n 5 Dayofweek: As we have said, during a week, the diagram varies a lot if it's a Sunday or a Monday (as we saw in our EDA) for example and every day of the week A Its own attributes and properties that allow you to know how much we will sell.\",\n",
       " 'Question 12: \\n  Show the percentage of observations with Customers greater than 1500 \\n  Show the value of the observation with the customers values greater than 7000 \\n  Calculate the linear correlation (Pearson) between Customers and Sales',\n",
       " 'We note a very biased distribution on the right for this variable with a significant difference between the average and the median. This is due to the dispersion of the data with a standard deviation of 7659, greater than the average and the median. \\n  It is preferable, from a realistic point of view, to introduce the median value into the three nan stores rather than the average, because it is biased by these aberrant values.',\n",
       " 'III. Data exploration',\n",
       " 'We can first see the 0.82 between customers and sales, which suggests that they are positively correlated as we have indicated above in the analysis.\\n\\n It is interesting to note that sales by client and promotion (0.28) are in fact positively correlated, since the organization of a promotion increases this figure.\\n\\n Customer sales are also correlated with the competition distance (0.21), positively, as I said, the greater the competition distance, the more sales by customer, which is logical , the stronger our competition, the more Rossman can reach a monopoly position in the region.\\n\\n In addition, the effect of promotion2 on customer sales, as we said above (0.22), caused a change in the purchase model and increased it when continuous promotion was applied .\\n\\n Finally, we can see that Storetype plays a major role in customer sales (0.44), which is probably due to the encoding of the type of store variable which suggests that the high categories like D which is equal to 4 have a higher weight. We must keep in mind that here we are on a linear correlation and it is necessary to take a step back from these results. For the analysis of the correlations of the categorical it would be necessary to look at the KHI two (allowing to highlight the intensity of the relationship between categories)',\n",
       " 'Question 20: \\n  Analyze graphics \\n  Do we have the same trends from year to year? \\n  Does the promo variable have an impact? \\n  Promo2? What can we deduce? Do we have the expected results?',\n",
       " 'Question 11: \\n  Show basic statistics for the custom variables \\n  Show the Boxplot Distribution \\n  Compare the two distributions between dirty customers (visual comparison)',\n",
       " 'Question 18: \\n  Analyze graphics \\n  Find the type of store with the greatest number of appearances in our dataset \\n  Find the type of store with the greatest number of sale \\n  Find the type of store with the highest average basket \\n  Is this coherent with the information you have about the different types of stores?',\n",
       " 'Question 29: \\n  Import the Save model \\n  Transform the X train and x train test data to spend 25 variables to N (N corresponding to the optimal number of features according to our variable selection algorithm) \\n  Show the error on test data',\n",
       " 'Question 33: \\n  Activate the GPU in Settings on your right. GPU Accelerator \\n  Redo the pipeline with a linear model https: scikit learn.org stable modules generated sklearn.linear model.ridgecv.html sklearn.linear model.ridgecv https: scikit learn.org stable modules generated sklearn.linear model.lassocv.html Sklearn Model.lassocv \\n  Make the pipeline with xgboostregressor',\n",
       " 'Here the best results are at the terminals of what we specified in Params. In order to find the best parameters it would be necessary to increase the number of parameters to tested. However, it would take too long and it requires a more powerful and accessible machine for several minutes to see several hours ...',\n",
       " 'This training game shows us that the type A store is the one with the largest number of branches, sales and customers in the four different types of stores. But that does not mean that it is the most efficient storeype.\\n\\n If we examine average sales and the number of customers, we see that in fact, it is the type B store which has recorded the highest average sales and the average number of the highest customers. One could assume that if B only has 17 stores but an average number of sales and customers so high that it is probably hyper Rossman branches when A would be smaller in size but much more present.\\n\\n Surprisingly, it is the type D store which has the highest average expenditure per customer, which is probably explained by an average distance of competition higher than the others, which means that each customer will buy more since he knows that there are not many similar shops in the surroundings.\\n\\n What would help us to better understand what is going on is to seek other variables explaining this behavior, such as assortments, competition and promotions.',\n",
       " 'Question 25: \\n  Separate our database into 2 samples Train Test 80 20',\n",
       " \"Question 31: \\n  Use a random forest with the following parameters n estimators = 128, criterion = 'mse', max depth = 20, min sample split = 10, n jobs = 1, random state = 35, verbose = 0\",\n",
       " 'The minimum is in Zero. This means that there are stores which were open but which have generated any turnover. We will leave in principle that these events are outliers. The open variable may be wrong a few times.',\n",
       " 'Question 5: \\n  Check that the sale are zero when the store is closed',\n",
       " 'We can observe similar trends in the column of customers and that of sales. In fact, our correlation factor of 0.82 explains that there is a strong positive correlation between sales and customers. In general, the more customers you have in a store, the higher your sales of the day. \\n\\n  We note that one day given, there have been a large number of customers in a store, due to a large promotion. These specific values affect the average, which explains the difference between an average of 762 and a median of 676. \\n\\n  We observe an asymmetry on the right in the two distributions due to the low number of aberrant values, but the high representation of each aberrant value only pushes the distribution to the right, as shown by the two histograms, which generally occurs when the average is greater than the median.',\n",
       " 'Question 3: \\n  Show the number of missing values for DF Train',\n",
       " 'Question 2: \\n  Show the first 5 lines of DF Train \\n  Show the number of lines and columns of DF Train \\n  Ditto for DF Store',\n",
       " 'Question 14: \\n  Fill the missing values from zero',\n",
       " 'Question 32: \\n  Show important variables from the previous model',\n",
       " 'Question 26: \\n  Define a function taking in input a classification, causing classifying it and automatically emerging the metric \\n  Use DummyRegressor to have a baseline',\n",
       " 'Given that the association of 0.1,2,3 with categorical variables such as Storetype, Assortment, Stateholiday affects the bias of the algorithm (0 would have less than 3 while in reality, Storetype A and Storetype It should be treated equally). We will simply convert it into a categorical now for the needs of the correlation analysis, then use the Get Dummies function to code them in a binary manner.',\n",
       " 'Question 35: \\n  Make the same with Catboost Using Randomizedsearchcv GPU \\n  Relaunch the model with the hyperparammeters obtained',\n",
       " 'Since the Competitionopensinceyear and Competitionopensincemonth variables have the same underlying meaning, merge them into a single variable that we call competition open allows the algorithm to understand the model more easily and creates fewer branches and therefore complex trees.',\n",
       " 'Stateholiday is not very important to distinguish (what type of vacation) and can be merged into a binary variable called is Holiday State.',\n",
       " 'What is interesting to trace is the effect of the closest competition distance on sales, to see if the one who has very distant competition actually achieves more sales than the one who has close competition. As the competition distance is a continuous variable, we must first convert it into a categorical variable with 5 different categories (I chose this number by looking at the distribution and to keep the aesthetics).',\n",
       " 'Variable Assortments \\n As we have indicated in the description, the assortments have three types and each store has a defined type and type of assortment: \\n  a means basic things \\n  B means extra. \\n  C means extensive things, so the greatest variety of products. \\n\\n What could be interesting is to see the relationship between a type of store and its type of respective assortment.',\n",
       " 'We already see a big difference, even at the level of the week (Monday to Friday), when we separate the promotion and the lack of promotion, and there is no promotion during the weekend.\\n\\n We can understand that Sunday has such a high peak, because very few stores open Sunday (only 33) if someone needs something urgent and does not have time to receive it during the week, he will have to make a certain journey to go to those open, even if it is not near his home. This means that these 33 stores open on Sundays in fact represent potential demand if all the Rossman Stores were closed on Sunday. This clearly shows us how important it is that stores are open on Sundays.\\n\\n After trying to examine the behavior of sales over a week over the years and months, I concluded that the scheme does not change, which means that there is always a peak on Monday with promotions, a Small peak on Friday before the weekend and a big peak on Sunday because of the iron stores',\n",
       " 'Random \\n\\n  His advantages: \\n \\n  The random journey times of the forests are quite fast, and they are able to process unbalanced and missing data. \\n  The process consisting in doing the average or combining the results of different decision trees makes it possible to overcome the problem of learning. \\n  They also do not require input data preparation. It is not necessary to put the data on a scale. \\n\\n  Its drawbacks: \\n  The main drawback of random forests is the size of the model. You might easily end up with a forest that takes hundreds of memory megaoctes and which is slow to assess. \\n  They become a little more difficult to interpret than ordinary decision -making trees, since we build forests of more than 50 decision trees and more using research by grid.',\n",
       " 'Question 28: \\n  Use the RFECV HTTPS variable selection function: SCIKIT Learn.org Stable Modules Generated Sklearn.FEATURE SELECTION.RFECV.HTML. With as parameter 1 \\n  This function maximizes a metric, outside in our case the best model is the one that minimizes metric. Propose an evolution of RMPSE in order to frame with the function',\n",
       " 'Question 6: \\n To avoid any bias, any noise in our dataset, we will delete this information. In addition, this will reduce the number of lines to be treated with our model. \\n\\n  Remove the days or stores are closed',\n",
       " '3) Promo2SinceWeek , Promo2SinceYear and PromoInterval ?',\n",
       " \"Variable Promotion \\n Let's see how the promotion affects Rossman's global sales by examining when there is a promotion and when there are not during these 3 years. This allows us first to see the impact of promotion and also to see the evolution of sales over specific years (therefore the trends of a given year) and the gradual increase in sales from 2013 to 2015:\",\n",
       " \"As you can see, some Lambda I were not included in the SLAU, and therefore, when solving X I, random uninformative meanings took it. Then it washes to help us\\n\\n Remark No. 2\\n\\n If someone has only one friend, then he cannot have more than one common friend with anyone.\\n\\nIn this case, if Jac {Ij} Neq 0 and lambda i = 1, then lambda j = frac {1} {jac {ij}}}\\n\\nNow we have found a lot of such people that will allow us to fill in almost all the passes in the massif number of friends. The remaining several unknown people can be found, for example, by hand.\\n\\nThe result of these operations is saved in the '.. Input Obj2VEC Friends People5 Manully.csv'\",\n",
       " 'And now we will solve the problem, it would seem, with the original connected very mediocre, namely: we will find at least one person for whom the list of his friends is fully known. The easiest way to do this is: many have only one friend, which means their list of friends is very simple. We assume that this lonely person has common friends exactly with J people (including himself). Then if someone has exactly one exactly J friends, then he is a friend of our lone comrade, and his list of friends looks like (0, 0 ... 0, 1, 0, ..., 0) T',\n",
       " ':',\n",
       " 'Data processing',\n",
       " 'Knowing the number of friends for each person, we can use the Frac {lambda i lambda j} {b {ij} = 1 frac {1} {jac {ij}}, but now for calculating b {ij} quad Forall. I, j.',\n",
       " 'Теперь нам известен один (i й) столбец матрицы S. Тогда:\\n\\n\\n UTU { } i = S i \\n\\n \\n begin{bmatrix}\\nu {11} u {12} ... u {1n} \\nu {21} u {22} ... u {2n} \\n... ... ... ... \\nu {n1} u {n2} ... u {nn} \\n end{bmatrix} begin{bmatrix}\\nt 1 0 ... 0 \\n0 t 2 ... 0 \\n... ... ... ... \\n0 ... 0 t n \\n end{bmatrix} begin{bmatrix}\\nu {i1} \\nu {i2} \\n... \\nu {in} \\n end{bmatrix} = begin{bmatrix}\\n0 \\n1 \\n... \\n0 \\n end{bmatrix} \\n begin{bmatrix}\\nu {11} u {12} ... u {1n} \\nu {21} u {22} ... u {2n} \\n... ... ... ... \\nu {n1} u {n2} ... u {nn} \\n end{bmatrix} begin{bmatrix}\\nu {i1} 0 ... 0 \\n0 u {i2} ... 0 \\n... ... ... ... \\n0 ... 0 u {in} \\n end{bmatrix} begin{bmatrix}\\nt 1 \\nt 2 \\n... \\nt n \\n end{bmatrix} = begin{bmatrix}\\n0 \\n1 \\n... \\n0 \\n end{bmatrix} \\n begin{bmatrix}\\nu {i1} 0 ... 0 \\n0 u {i2} ... 0 \\n... ... ... ... \\n0 ... 0 u {in} \\n end{bmatrix} begin{bmatrix}\\nt 1 \\nt 2 \\n... \\nt n \\n end{bmatrix} = begin{bmatrix}\\nu {j1} \\nu {j2} \\n... \\nu {jn} \\n end{bmatrix} \\n begin{bmatrix}\\nt 1 \\nt 2 \\n... \\nt n \\n end{bmatrix} = begin{bmatrix}\\nu {j1} u {i1} \\nu {j2} u {i2} \\n... \\nu {jn} u {in} \\n end{bmatrix}',\n",
       " 'It would seem that the task of finding the exact number of friends has already been resolved. But no:',\n",
       " 'Again, enter the designations: \\n\\n  B = (b {ij}), s = (s {ij}), where s {ij} = 1, if a person I is friends and 0 otherwise. Actually, our task is to find the matrix S despite the fact that B has already been found. \\n\\n  Remark 3 \\n\\n  Ss = b (matrix S in a square is b) \\n\\n Consider the singular decomposition of the matrix S: \\n\\n  S = UTU (matrices made up of their own vectors coincide due to symmetry S). Then b = uttu. That is, their own vectors for B and S coincide. But we now know to find all its own meanings to find S.',\n",
       " \"We denote B {IJ} The number of common friends in people with numbers I and J, Lambda I number of friends in humans I.\\n\\nThen it’s easy to see that:\\n jac {ij} = frac {b {ij}} {lambda i lambda j b {ij}}, which means frac {lambda i lambda j} {ij} = 1 frac {1} {ij}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}\\n\\n Remark No. 1\\n\\n If JAC {IJ} (Jacar's coefficient for people with numbers I and J) is quite small, but different from 0, then it is likely that these people have exactly one friend.\\n\\nThen the last equality for some couples (i, j) turns into lambda i lambda j = 1 frac {1} {jac {ij}}. In fact, we get a sludge for Lambda I, I in [1, N], which does not have an exact solution, since pairs with Jac {IJ} 0.02 are more than the number of people. This kind of systems are well solved by the method of smallest squares.\",\n",
       " \"Everything. We found our own meanings and considered the original matrix on them. Let's check the accuracy of the resulting solution:\",\n",
       " 'PREDICTING A NEW RESULT',\n",
       " 'TRAINING DECISION TREE REGRESSION MODEL ON DATASET',\n",
       " 'IMPORTING DATASET',\n",
       " 'IMPORTING LIBRARIES',\n",
       " 'VISUALISING THE DECISION TREE REGRESSION RESULTS',\n",
       " \"Setup of the data \\n Let's turn the data into pandas dataframes. Also, let's rename some columns to get easier and in DF 1 and DF 2 dataframes, let's use the names of countries like Dataframe indexes.\",\n",
       " 'Don Project Challenge Happiness 2021 \\n A worldwide view and a slightly deeper analysis of Brazil in the 2021 world happiness report. \\n FAQ: https: worldhappiness.report faq \\n\\n Projects that inspired us: \\n  https: www.kaggle.com Joshuaswords Awesome Eda 2021 Happiness Population \\n  https: www.kaggle.com ajaypalsinghlo World Happiness Report 2021 \\n\\n  Introduction \\n\\n The main intention of this project is to learn more about the area of data science and test our knowledge through publications and knowledge exchange. The theme of this project is how the world is in relation to happiness and, in particular, how Brazil is in this scenario compared to others. The notebook is based on the 2021 and previous years world happiness report.',\n",
       " 'Importing Data',\n",
       " 'Training',\n",
       " 'Importing Packages',\n",
       " 'Model evaluation',\n",
       " 'Data generation',\n",
       " 'CNN MODEL',\n",
       " 'Fitting',\n",
       " 'Data position',\n",
       " 'Armo CSV for submitting',\n",
       " 'We teach a neural network',\n",
       " 'We normalize the data',\n",
       " 'Data preparation',\n",
       " 'We create a neural network',\n",
       " 'Recognition of objects on images from a set of data Cifar 10 \\n\\n Training course [Programming of deep neural networks on python] (https: openedu.ru cousse urfu pydnn).',\n",
       " 'We convert the correct answers to the One Hot Encoding format',\n",
       " 'Dataset Metadata \\n Here we will upload the Urbansound meta data .csv file to the panda data frame.',\n",
       " 'X (Data) → Sound Time Series \\n\\n  SR (Sample Rate) → Sound Frequency (Hz) \\n\\n You can also install the sound frequency by changing the sound.',\n",
       " 'Class distributions',\n",
       " 'We will upload an example from each class and visually examine the data for any mold. We will use Librosa.display and Matplotlib to download Librosa later to upload the audio file into a array.',\n",
       " 'These sound quotes are digital audio files in the .wav format.',\n",
       " 'Bit Depth \\n\\n It also has a wide variety of lice depths. For a certain bit depth, we may need to normalize them by taking maximum and minimum amplitude values.',\n",
       " 'Audio Channels \\n Most of the samples have two sound channels (in the sense of stereo) and a few are only single -channel (mono). \\n\\n The easiest way to make them uniform is to combine the two channels in steroid samples in one channel by taking the average of the values of the two channels.',\n",
       " 'Removal of MFCCs for each file \\n\\n Now we will remove a MFCC for each audio file in the data set and store it in a panda data frame with the classification label.',\n",
       " '1. Data Exploration and Visualisation',\n",
       " 'Voice Features that require normalization \\n  Audio Channels \\n  SAMPLE RATE \\n  Bit Depth \\n\\n We will continue to use Librosa, which will be useful for pre -processing and features.',\n",
       " '2. Data Preprocessing and Data Splitting',\n",
       " 'Feature Extraction Refinement \\n  In the previous property subtraction phase, MFCC vectors will vary in size for different audio files (depending on the duration of the samples). \\n  However, CNNs require a fixed size for all inputs. To overcome this, we will reset the output vectors and make them all in the same size.',\n",
       " '3. Model Training and Evaluation',\n",
       " 'Audio Sample File Properties \\n We will repeat each of the audio sample files and remove the number of sound channels, sampling speed and bit depth.',\n",
       " 'Initial Model Architecture MLP\\n Using Keras and Tensorflow rear end, we will begin to create a multi -layered sensor (MLP) nervous network.\\n We will start with a simple model architecture of three layers, an input layer, a hidden layer and a layer of output.\\n The first layer will take the form of input. Since each example contains 40 mfcc (or column) (1x40), we have a shape, which means that we will start with 40 input types.\\n The first two layers will have 256 nodes. The activation function that we will use for our first 2 layers is RELU or a straightened linear activation. This activation function has been proven to work well in neural networks.\\n In addition, we will apply a 50th quit value to our first two layers. This will randomly exclude the nodes from each update cycle, which will result in a network that can generalize better and less likely to generalize education data less.\\n Our output layer will have 10 nodes (number labels) that match the number of possible classifications. The activation is for softmax of our output layer. Softmax increases the sum of the output to 1, so that the output can be interpreted as probability. The model will then predict which option has the highest probability.',\n",
       " \"Bit Depth \\n\\n Librosa's installation function also normalizes the data and its values vary between 1 and 1. This eliminates the complexity of the data set with a wide range of lice depths.\",\n",
       " 'Predictions',\n",
       " 'Test The Model \\n Here we will review the accuracy of the model in both training and test data sets.',\n",
       " 'Other audio features to be taken into consideration \\n\\n At this stage, it is not yet clear whether other factors such as sample time length and volume levels should be taken into account. \\n\\n In the meantime, we will continue as it is and considering the validity of our target measurements, we will come back to address them later.',\n",
       " 'Test The Model \\n Here we will review the accuracy of the model in both training and test data sets.',\n",
       " 'Training \\n Here we will train the model. Since a CNN training may take a significant amount of time, we will start with a low number of periods and a low party size. If we can see that the model is converged, we will increase both numbers.',\n",
       " '4. MODEL REFINEMENT \\n\\n In our first attempt, we succeeded in obtaining a classification accuracy score as follows: \\n\\n Training data accuracy: 92,3 \\n\\n Test data accuracy: 87 \\n\\n Now we will see if we can develop this score by using a Convolutional Neural Network (CNN).',\n",
       " 'Confusion Matrix',\n",
       " 'Predictions',\n",
       " 'Training',\n",
       " 'Urban Sound dataset Air Conditioner Car Horn Children Playing Dog bark Drilling Engine Idling Gunshot Jackhammer Siren Street Music',\n",
       " \"Sample Rate Conversion \\n\\n By default, Librosa's load function converts the sampling ratio to 22.05 kHz, which we can use as our comparison level.\",\n",
       " 'Merge Audio Channels \\n\\n Librosa will also convert the signal into mono, ie the number of channels will always be 1.',\n",
       " 'We will use the following libraries for sound analysis: \\n\\n 1. ipython.display.audio \\n\\n  This allows us to play the sound directly in the jupyter notebook. \\n \\n 2. Librosa \\n\\n  Librosa is a Python package for music and sound processing by Brian McFee and allows us to upload sound to our notebook as a series of numb for analysis and manipulation.',\n",
       " 'We will use ipython.display.audio to play audio files, so we can examine it as an auditory.',\n",
       " '[Original Notebook] (HTTPS: Github.com Mikesmalas Uditation ML Capstone) is prepared with help from this notebook.',\n",
       " 'EXTRACT FEATURES \\n\\n MFCC summarizes the frequency distribution along the window size, so that it is possible to analyze both frequency and time properties of sound. These voice representations will allow us to determine the properties for classification. \\n\\n For this, we will use the MFCC () function of Librosa, which forms a MFCC from the time series sound data.',\n",
       " 'This indicates that Librosa calculates a number of 40 mfcc on 173 frames.',\n",
       " 'Convolutional Neural Neural Network (CNN) Model Architecture\\n We will replace our model to be a evolved nervous network (CNN) using Keras and Tensorflow rear end.\\n Evolution layers are designed for property detection. It works by sliding a filter window above the input and storing the result on a feature map. This process is known as evolution.\\n The Filter parameter indicates the number of nodes in each layer. While the size of each layer increases from 16, 32, 64 to 128, the kernel parameter indicates the size of the core window and then results in a 2x2 filter matrix.\\n The first layer (40, 174, 1) will take the form of input, where the number of 40, MFCCs. 174 is the number of frames that take into account filling and indicates that 1 is mono.\\n The activation function that we will use for our evolution layers is the same as our previous model. We will use a smaller quitting value of 20 in our evolution layers.\\n Each evolution layer has a maxpooleing2D -related pool layer with the latest evolution layer with a globalaveragepooling2D species. The ponding layer reduces the size of the model (reducing parameters and relevant calculation requirements) that serves to shorten the training time and reduce excessive compliance. The maximum type of pooling takes the maximum size for each window, and the overall average type of pooling type is suitable for feeding to our intense output layer.\\n Our output layer will have 10 nodes (number labels) that match the number of possible classifications. Activation is for softmax of our output layer. Softmax increases the sum of the output to 1, so that the output can be interpreted as probability. The model will then predict which option has the highest probability.',\n",
       " 'Split The Dataset \\n Here we will use Sklearn.model Selection.Train test split to divide the data set into training and test sets. The test set size will be 20 and we will determine a random situation.',\n",
       " 'SAMPLE RATE \\n\\n There are a wide variety of sample rates used in all worrying samples (ranging from 96K to 8K). \\n\\n This means that we need to apply a sampling ratio -conversion technique (up or down transforming), so that we can see a agnostic representation of waveforms that allow us to make a fair comparison.',\n",
       " 'Sound waves are sampled at separate intervals known as sampling speed (typically 44.1 KHz for CD quality sound, ie 44,100 times per second). \\n\\n Each example is the amplitude of the wave in a particular period of time, where the bit depth, for example, determines how detailed it will be known as the dynamic range of the signal (typically 16 bit, which means that it can vary from a sample 65,536 amplitude).',\n",
       " 'Convert the Data and Labels \\n\\n We will use Sklearn.Prepprocessing.labelencoder to encode categorical text data to understandable digital data by the model.',\n",
       " 'Compiling the model \\n\\n We will use the following three parameters to compile our model: \\n  We will use the lost function categorical Crossentropy. This is the most common choice for classification. A lower score indicates that the model performs better. \\n  When educating the metrics model, we will use the measure of accuracy that will allow us to view the accuracy score in verification data. \\n  Optimizer here we will use the Man, which is usually a good optimizing man for many usage situations.',\n",
       " 'Compiling the model \\n\\n To compile our model, we will use the same three parameters as the previous model:',\n",
       " 'Here we can see that class labels are unstable. Although 7 out of 10 classes are exactly 1000 examples and are not far away from the siren 929, the remaining two (Car Horn, Gun Shot) have a significant extent less than 43 and 37 respectively.',\n",
       " 'Observations \\n  Without a visual examination, we can see that it is difficult to visualize the difference between some classes. \\n  Especially for air conditioning, drilling, motor idle and repetitive sounds for crusher, waveforms are similar. \\n  Similarly, the peak in the dog barking sample is similar to the sample that was hit by a gun (samples differ in two peaks for two guns for two weapons for a dog for a dog barking). In addition, car horn is similar. \\n  There are similarities between children playing with street music. \\n  It will naturally perceive the difference between the human ear and harmonics, and it would be interesting to see how well a deep learning model can produce the characteristics necessary to separate these classes. \\n  However, it is easy to distinguish the difference between the form of wave shape between certain classes such as dog barking and crusher.',\n",
       " 'Univariate Analysis',\n",
       " 'Bivariate Analysis',\n",
       " \"Let's do a grouping process according to our target variable Variety, let's view the average of our variable values.\",\n",
       " \"Let's print the maximum value of sepal.length.\",\n",
       " \"With Petal Length collecting Sepal Length values, let's create a new Total Length subjectivity.\",\n",
       " \"With Value Counts () function, let's question how balanced our data frame is distributed.\",\n",
       " \"Let's view the first 5 observations of the data frame.\",\n",
       " \"Let's draw a heat map to better read the correlation coefficients.\",\n",
       " \"Let's visualize the same two data with ScatterPlot, but this time, with the variety parameter, we break it according to the target variable. \\n\\n Can a clustering be made with sepal variables between 3 different colors? Let's think about how distinct it can be distinguished.\",\n",
       " \"In the data frame, we see that sepal.width and sepal.length variables are continuous. To visualize these two continuous data, let's use scatterPlot first.\",\n",
       " \"According to our target variable Variety, let's print the standard deviation values of our Petal.length variable by grouping.\",\n",
       " 'Which flower type escapes from the type of observation of our data frame? \\n\\n We have already seen with Value counts that it is 50 x 3 and balanced, but to visually express it, the SNS.CountPLOT () function can be given the variety parameter.',\n",
       " \"In order to better understand, let's draw a distipot on sepal.width.\",\n",
       " \"Let's give a 3rd dimension by adding the Hue = variety parameter to the same visualization.\",\n",
       " \"Let's print the standard deviation value of Total.length.\",\n",
       " \"Let's view how many unique values of the target variable of our data frame contains.\",\n",
       " \"Let's install the libraries that we will use in line with our needs below.\",\n",
       " \"Let's view how many attributes and how many observations of the data frame.\",\n",
       " 'Reinforcement of exploratory data analysis skills',\n",
       " \"To reinforce the answer to this question, let's print the correlation coefficient between the two variables.\",\n",
       " \"Let's add the Kind = Kde parameter to the visualization we have done in the previous cell. In this way, we will see that the distribution has become a dotted demonstration and turned into a density -oriented visualization.\",\n",
       " 'Thank you for your effort, time and interest.',\n",
       " \"To examine the same two data with frequencies from a different angle, let's visualize it using JointPlot.\",\n",
       " \"Let's print the average value of Total.length.\",\n",
       " \"SNS.LMPLOT () visualization with petal.length and petal.width variables. What kind of relationship is there between Petal Length and Petal Width? Let's answer the question.\",\n",
       " \"For numerical variables in the data frame, let's view the basic statistics values. \\n\\n Let's make an idea of which variables have a variables and how much variance they have from standard deviation and average values.\",\n",
       " \"For three flowers, let's visualize three different violin graphs with a single line on the distribution of sepal.length variable.\",\n",
       " \"Let's view the variety unique values of the target variable of our data frame.\",\n",
       " \"With ScatterPLOT, let's draw the distribution of Petal.length and Petal.width variables.\",\n",
       " \"Let's observe how many missing value in which attribute is in the data framework.\",\n",
       " \"Let's upload our data frame from the knee we are in and assign it to the DF variable by turning it into a data frame. (PD.Read CSV (... CSV))\",\n",
       " \"Let's print sepal.length observations of Setosa greater than 5.5.\",\n",
       " 'Examine the distribution of sepal.width variable by drawing the violin graph. \\n\\n What does the distribution mean to us, can we say that it is a normal distribution?',\n",
       " \"To show if there is a correlation between the numerical variables, let's draw the correlation matrix. Let's have an idea about the correlation coefficients. \\n\\n Which two variables are the most powerful positive relationship?\",\n",
       " \"Let's view which type of variables in the data frame and memory usage.\",\n",
       " \"Let's visualize the sepal.length and sepal.width variables with SNS.JaintPlot, and examine the regions where distribution and distribution are high frequency.\",\n",
       " \"Let's print the sepal.length and sepal.width variables and values of observations with petal.length less than 5 and virginica.\",\n",
       " 'Temporary table creation 2018.1 2020 monthly data \\n  The temporary table seems to be deleted when an error. If you get an error during the cell execution, make a temporary table.',\n",
       " 'Society for Learning Trade Statistics 2 (HTTPS: Scrapbox.io Manabiai Lesson) teaching materials \\n\\n  (Assignment) Mask related from January to June 2020 \\n\\n\\n  Self -study task \\n  Which country did you buy a mask manufacturing machine? Where are the customs? \\n  Which country do you buy a mask material? Where are the customs? \\n  Which country is the import of masks?',\n",
       " 'Checking for unique patients',\n",
       " 'Observe the trend of the number of leases with the change of humidity, and take the average value of the lease according to the humidity.',\n",
       " 'Fill in it and draw a picture to observe the density distribution of these four feature values',\n",
       " 'As can be seen: \\n\\n  1. The lease of bicycles in shared bicycles increased in 2012 as a whole increased compared to 2011; \\n  2. The lease situation fluctuates obvious with the month; \\n  3. Data fluctuate violently from September to December 2011, March to September 2012; \\n  4. There are many local valley values. \\n\\n\\n  3.2.5 impact on the number of travel people \\n\\n There are many local troughs in the data in the figure above, so the lease number is displayed in the median number according to the season, and at the same time observe the temperature changes in the season',\n",
       " '1. Working day: \\n\\n Due to the difference between the number of days and the days of rest and rest, the average value of the number of leases on working days and non -working days is available.',\n",
       " 'It can be seen that the number of temporary users and members of members ushered in the peak in autumn, and the lowest number of users in the spring season \\n\\n\\n\\n  3.2.6 The impact of weather conditions on travel conditions \\n\\n Considering that the number of days in different weather is different, for example, very bad weather (4) will rarely appear. Check the number of data numbers at different weather levels, and then take the average value of the weather level at the level of the weather.',\n",
       " 'Kaggle shared bicycle project visualization',\n",
       " '2. Prepare data \\n  2.1 Check the loss of missing values',\n",
       " 'It can be seen that the wind speed fluctuates and large between September 2011 and December 2011 to March 2012. Observe the trend of the number of leased persons with the wind speed. Considering that the wind speed is particularly large, if the average value is taken, it will appear. Abnormal, so take the maximum value of the lease at the wind speed.',\n",
       " \"Because this data is at the peak of the commute, it is indeed an abnormal data \\n\\n\\n\\n  3.2.7 The effect of wind speed on travel conditions \\n\\n Let's take a look at the trend of wind speed in two years\",\n",
       " 'There is no missing data in this dataset, but it does not mean that there is no abnormality. \\n\\n\\n  2.2 Check the abnormal value',\n",
       " 'There are a lot of travel people when we have observed weather level 4, especially the number of members of members is even higher than the average value of weather level 2. This is a bit challenging my cognition. Is this record just at the peak of get off work? \\n\\n\\n Because of the data of the known lease, there is only one data of the weather level 4, so I want to print out the data of the weather level 4 to observe',\n",
       " 'Starting from numerical type data, you can see that the count value is largely different, so I hope to observe their density distribution',\n",
       " '1. Data collection \\n  project instruction  \\n  Data content description \\n  submit questions  \\n  Variable description \\n 2. Data preparation \\n  Check the missing value \\n  Check and deal with abnormal values \\n 3. Analysis data \\n  Overall observation \\n  One -to -one display \\n 4. Select feature value \\n 5. Select models, training models \\n 6. Forecast test set data',\n",
       " 'Look at the density distribution after the data other than 3 standard deviations',\n",
       " '3. Analysis data: \\n\\n Visualized and observed data \\n\\n  3.1 October observation \\n\\n The problem is to predict the total lease amount per hour. First, look at the relationship between the three values related to the lease amount and other feature values.',\n",
       " 'It can be seen that the larger the lease speed, the less leased the lease, the significantly decreased when the wind speed exceeds 30, but the wind speed at about 40 has a rebound. It should be the same as the weather conditions. a bit',\n",
       " \"You can see through comparison: \\n\\n  1. On the working day, there are two peaks for members of members on and off, and there will be a small peak at noon. It is speculated that it may be a lunch; \\n  2. The ups and downs of temporary users are relatively smooth, and the peak period is around 17 o'clock; \\n  3. And the number of members of member users far exceeds temporary users. \\n  4. For the non -working day, the lease number is a normal distribution over time. The peak is around 14 o'clock, the trough is around 4 o'clock, and the distribution is relatively uniform. \\n\\n\\n  3.2.2 The effect of temperature on the number of leases \\n\\n Observe the trend of temperature first to observe the trend of temperature first\",\n",
       " 'It can be observed that the number of rising cars with temperatures generally shows an upward trend, but when the temperature exceeds 35, it begins to decline, reaching the lowest point when the temperature is 4 degrees. \\n\\n\\n  3.2.3 The impact of humidity on the number of leases \\n\\n Observe the trend of humidity first',\n",
       " 'It can be seen that the temperature trend of each year is the same as the monthly change. In July, the temperature is the highest in July, and the temperature in January is the lowest. Then take a look',\n",
       " 'It is also an abnormal value of the peak of get off work \\n\\n\\n\\n  3.2.8 date of the impact on travel \\n\\n Considering whether the same date is working day, the week, and the year of the affiliation is the same.',\n",
       " 'Because in the end, the random forest prediction is needed, and in order to facilitate the visual data, the DateTime is split into the date, period, year, month, and Friday column',\n",
       " 'The number of days of holidays accounts for a very small number of days, so the average value of the holiday and non -holiday days',\n",
       " '5. Select models, training models',\n",
       " 'You can observe the number of leases at about 20 humidity reaching its peak value, and then slowly decreased. \\n\\n\\n  3.2.4 years of impact on the number of leases in the month \\n\\n Observe the trend of the total number of car rental over time in two years.',\n",
       " 'After the number transformation is transformed, the data distribution is more uniform, and the size differences are reduced. Using such a label is good for the training model. \\n\\n\\n\\n Next, process the remaining numerical data, because other data is also included in two data sets at the same time, and the two data sets are merged to facilitate data processing.',\n",
       " 'It can be seen from the above two pictures: \\n\\n  1. The number of membership users on working days is large, and the number of temporary users is small; \\n  2. The number of membership users in the weekend decreases, and the number of temporary user leasing increases. \\n\\n\\n  2. Holiday \\n\\n Because the number of holidays is very small during the year, first look at the annual holidays for a few days first,',\n",
       " 'It can be seen that the impact of characteristic values on the number of leases is. During the period of time and humidity, the weather level weather level wind speed during the period of the month is the day of the weather level. \\n\\n\\n\\n  3.2 Show by item \\n\\n  3.2.1 The impact on the number of leases during the period \\n\\n Because the impact of time period on the number of leases is the biggest first display of this data',\n",
       " 'Separation training set and test set',\n",
       " 'It can be seen about: \\n\\n  1. Members travel too much on working days, have less travel on holidays, and temporary users are opposite; \\n  2. The number of travels in the first quarter is generally less; \\n  3. The number of leases decreases with the level of weather; \\n  4. Hours have significant impact on the lease situation. Members present two peaks, and non -members present a normal distribution; \\n  5. The number of leases decreases with the increase in wind speed; \\n  6. Temperature and humidity have a greater impact on non -members and have a small impact on members \\n\\n\\n Let ’s check the correlation between each characteristic and the total amount of car rental (count) per hour. Because the above figure can be seen that the characteristic value is basically linearly related to the number of car rental, so find their linear correlation coefficient',\n",
       " 'Use random forest filling wind speed',\n",
       " \"1.1 Project description\\n\\n The bicycle sharing system is a method of renting a bicycle. Registered members, car rental, and car return will automatically complete the network network network network network. Through this system, people can rent a bicycle from one place and ride to their destination. return.\\n\\n In this competition, participants need to combine the mode of use in historical weather to predict the D.C. Washington Capital Bicycle Sharing Project for bicycle rental needs.\\n\\n\\n\\n 1.2 Data content description\\n\\n The competition provides a two -year lease data that spans two years, including weather information and date information.\\n\\n The training set consists of data from the first 19 days of each month. The test set is data from the twentieth day of each month to the end of the month.\\n\\n\\n\\n 1.3 Raise questions\\n\\n The number of members' leases, temporary leases and total leases are predicted by testing concentrated weather and other feature values.\\n\\n\\n\\n 1.4 variable description\\n\\n DateTime (Date) Hourly Date TimesStamp\\n\\n Season (Season) 1 = Spring, 2 = Summer, 3 = Fall, 4 = Winter\\n\\n Holiday (whether holidays) WHITHER the Day is Considered a Holiday\\n\\n WORKINGDAY (Whether to work day) Whither The Day is Neither a Weekend NOR HOLIDAY\\n\\n Weather (weather level)\\n\\n 1. Clear, small clouds, cloudy.\\n\\n 2. Fog cloudy days, fog crushed clouds, fogless clouds, fog\\n\\n 3. Little snow, light rain thunderstorms, light rain clouds\\n\\n 4. Rain rain, hail thunderstorms, snow fog\\n\\n Temp (temperature) Temperature in Celsius\\n\\n ATEMP\\n\\n HUMIDITY (relative humidity) Relative Humidity\\n\\n WindSpeed \\u200b\\u200b(Wind) Wind Speed\\n\\n Casual (number of temporary leases) Number of Non RegisterEd User RENTALS Initiated\\n\\n Registered\\n\\n Count (total lease) Number of Total Rentals\",\n",
       " 'Through this distribution, some problems can be found, such as why wind speeds 0 data, and observation statistics and descriptions found that the vacancies are between 16. From here, it may be speculated that the data itself may be missing, but it is filled with 0 to fill in it. However, these wind speeds are 0 data. It is hoped to use random forests to fill in the lack of wind speed according to the same years, month, season, temperature, and humidity according to the same year, month, season, temperature, and humidity. \\n\\n Look at the description statistics of non -zero data before filling.',\n",
       " 'Separation training set and verification set',\n",
       " 'It is found that the diabetic distribution of the data density is serious and there is a long tail, so I hope to process the long tail of this column of data and eliminate the data other than the three standard deviations.',\n",
       " '4. Select feature value \\n\\n According to the previous observations, decide the time period, temperature (TEMP), Humidity, year (year), month (Month), Season, Weather level (WindSpeed RFR), weekly (Windspeed RFR), weekly Weekday, whether working days (workingday), whether holidays, 11 items',\n",
       " 'It can be seen that the data fluctuations are still very large, and we hope that fluctuations are relatively stable, otherwise it is easy to produce it. \\n\\n\\n Therefore, I hope to change the data so that the data is relatively stable, and the number of changes to the number is selected to complete this task.',\n",
       " '6. Forecast test set data',\n",
       " 'It was filled with random forest.',\n",
       " 'Observe the data above, you can see the four columns of Temp (temperature), ATEMP, HUMIDITY (humidity), and WindSpeed (wind speed) also belong to numerical data, and you can also check their distribution.',\n",
       " 'Observe the statistical description of the lease amount after removing 3 standard deviations',\n",
       " '2. Learning curves \\n Metrics are key figures for individual classifiers. Curves arise when we look at a crowd of classifiers. Learning curves are a plot of the metric against the available amount of training data. In particular, they provide information about whether there are enough training data. But overfitting can also be better understood.',\n",
       " '2. Regularization \\n Classifiers often have so -called hyperparameters. These are parameters whose values are not learned by the .Fit method. They change the classifier and must be determined by them. With the KNN classifier, the parameter K is a hyperparameter. \\n Hyperparameters are relevant for overfitting because you can restrict the thickness of a classification (the BIAS is increased see [Bias Variance Tradeoff] (https: en.wikipedia.org wiki bias e2 80 93variance Tradeoff)). Less powerful classifiers learn simpler structures and overforts less. Therefore, regularization is one of the most important measures against overfitting. It is not entirely clear which value range of a hyperparameter, which means weaker regularization. \\n  [] (http :) For this we evaluate two different KNN classifiers on the training and validation data.',\n",
       " 'Also add online what exactly Train Test Split does. What returns Train Test Split (X, Y, Train Size = 0.9, Stratify = Y), or Train Test Split (X, Y, X, Y)?',\n",
       " 'Now we draw the learning curves',\n",
       " '1. metrics \\n This section shows a (bad) example of Machine Learning: A model is learned that overfits. Which key figures help us to recognize such grievances?',\n",
       " 'Task: Find another metric (F1 Score, for Rate, Recall, ...), research their documentation in Scikit Learn and evaluate it on the validation data set.',\n",
       " 'Please evaluate the two cells above several times. Do you see how much the curves change (why not at KNN)? These curves are random size. \\n It is therefore better to indicate an average and a standard deviation for these curves instead of a single implementation. This is shown in the following cell: \\n The code comes from [here] (https: scikit lear.org stable auto examples model selection plot validation curve.html).',\n",
       " 'In order to recognize overfitting, we need validation and test data. This is what we measure our generalization ability, whereby (because of data snooping, see below), validation accuracy can still easily overestimate generalization. \\n This can also happen on the test data (although the classifier never has this data), e.g. if new data structures appear in the productive use of the prototype. How dramatic and tragic that can be has shown the [Uber accident of a self -driving car, only for people with good nerves] (https: www.youtube.com Watch? V = rasbcc4yooo). The machine learning engineers thought that its prototype was already so good that you can leave it on the street for test drives ...',\n",
       " 'Here we overfort a KNN classifier:',\n",
       " '1. Data in training and validation set (or test set) split \\n So far we have worked with an artificially generated data set and did as if we could generate any number of data. \\n In practice, of course, this is different. Typically, you only have one data record there and have to decide for yourself how many data you put aside for training, validation and testing. Here you will learn how to split a data record. \\n So we start with just one data record x, y:',\n",
       " 'In turn, discuss the overfitting topic. Personally, I find the fact that the cross validation score for Max Depth 6 decreases remarkable. I have never seen this in artificial data records.',\n",
       " 'The following cells show how a data set is split into training and test data.',\n",
       " 'Task 1: \\n Which one is better on our training data record? KNN1 or KNN50? Think about before evaluating the following cell',\n",
       " \"But we know that the training accuracy is not relevant is more important: which classification is better on a validation data set? \\n\\n  Exercise 2: Do you summarize again: Kley small or large values of K rather overfitting? What about Max Depth's values for a decision tree? Explain \\n  Task 2: Which classifier is better on our validation data? KNN1 or KNN50? Fill out the two arguments of the four lines in the next cell.\",\n",
       " 'Parameter curves for K NN',\n",
       " 'The problem of overfitting is more subtle than you would have already removed it through a simple train test splitting: by looking at your data (or optimizing hyperparameter values \\u200b\\u200bon the same validation data several times), you may already internalize information that helps you on this Validation data set to get better metric values. One speaks here of [data snooping] (https: web.ma.utexas.edu users mks statmistakes datasnooping.html).\\n\\nThe solution: put a test set aside as soon as you have received the data. Ideally, the test data are only there for one purpose: to measure the metrics of the final for the practical use (without improving it again). The decision about opening the test data is (sensibly) a management decision because it cannot be reversed.\\nIn practice, there are sometimes not enough data to be able to form a test set. Overfitting and Data Snooping Danger increases considerably.',\n",
       " 'You do not have to understand the following lines in detail. You create an artificial 2 dimensional data record (Features X and Label Y). But it is certainly useful to know [Sklearn.datastets] (https: scikit lear.org stable modules classes.html modules sklearn.datastets).',\n",
       " 'Then let yourself be spent on a classification report and find out what it is.',\n",
       " 'Please correct the code so that a useful report is output.',\n",
       " 'But maybe you want to use a different metric? Scikit Learn provides a lot, e.g. the Confusion Matrix, the Precision, or AUC, etc. [Find out] (https: Scikit Learn.org Stable Modules Classes.html Sklearn Metrics Metrics)',\n",
       " 'Training accuracy is significantly higher than the validation accuracy. This difference shows us overfitting. In particular, we should not use the training accuracy to assess how good our classification is. On the other hand, if we take the validation accuracy, we get a significantly better picture of reality, i.e. the accuracy that our classifier will have on new data.',\n",
       " 'The Random State in the Code cell is trying to prevent data snooping. See below, subchapter 3.',\n",
       " '3. Data Snooping',\n",
       " \"Metrics in Scikit Learn \\n So far we had used its Clf.Score (X, Y) method to evaluate a classifier CLF. I always have a bit of queasy because I don't know what score is. Of course it says in the documentation:\",\n",
       " 'If you perform the following line without a fixed random state, you will see all your data over time, which can lead to overfitting on the validation data: Possibly you will experience a nasty surprise later when the test data is opened. By always training on the same data with a fixed random state, this can be partially prevented.',\n",
       " 'Measures against overfitting \\n We present various solutions and strategies for avoiding overfitting. Especially: \\n\\n 1. Spled -off of a validation and test data set to measure generalization ability. \\n 2. Regularization \\n 3. Fixing of the Random State to further prevent data snooping \\n\\n 4. Cross validation for small data sets \\n 5. Parameter curves for determining suitable hyperparameter values',\n",
       " 'Parameter curves for decision -making trees',\n",
       " '4. Cross validation \\n If you calculate your training accuracy with Accuracy Score, you will receive a so -called point estimate, i.e. a value. Sometimes, however, they want an interval estimate, i.e. a mean and a confidence interval (or otherwise a stray, such as the standard deviation). Cross validation is a way to generate several values:',\n",
       " 'Aus statistischer Sicht ist das obige Vorgehen solide, und ich empfehle diese Vorgehensweise. Trotzdem sollten Sie sich bewusst sein, dass Kreuzvalidierung den Verallgemeinerungsfehler unterschätzen kann. Eine Passage aus einem [Nature Artikel von Tabe Bordbar et al](https: www.nature.com articles s41598 018 24937 4):\\n\\n CV results may depend on similarity of test and training sets. Consider first a contrived and extreme scenario where each data sample is present in many copies in the available dataset, so that in any random partition during CV (the standard way to construct training and test sets) a test sample is likely to have a copy of itself present in the training set. A supervised learning method may predict accurately on such a sample since it has already seen that sample during training. In this case, the high predictive accuracy of the model in the CV evaluation may simply be the result of this proximity between training and test points (e.g., by a successful adoption of the nearest neighbor approach)10, and does not necessarily imply an accurate encapsulation of the input output relationship in the trained model. (...) Thus, for the same reason as above, the accuracy estimated by CV on random partitions – as opposed to other partitioning approaches (Fig. 1d) – may be misleading and not generalize to more dissimilar experimental conditions. \\n\\nÜberlegen Sie auch, wie aufwändig Kreuzvalidierung in Bezug auf die Rechenkapazität ist. Denken Sie dabei insbesondere auch an Deep Learning. Das sind neuronale Netze, deren Training Stunden, wenn nicht Tage dauern kann.',\n",
       " 'In order to recognize overfitting, we need validation and test data. This is what we measure our generalization, whereby the validation accuracy can still easily overestimate generalization (because of data snooping, see below). \\n This can also happen on the test data (although the classifier never has this data), e.g. if new data structures appear in the productive use of the prototype. How dramatic and tragic that can be has shown the [Uber accident of a self -driving car, only for people with good nerves] (https: www.youtube.com Watch? V = rasbcc4yooo). The machine learning engineers thought that its prototype was already so good that you can leave it on the street for test drives ...',\n",
       " 'Task: Determine it not only a sample value for the accuracy, but a real measured value, including error specification',\n",
       " '5. Parameterkurven',\n",
       " 'Tasks: : \\n  Why is the training curve falling? \\n  Why is the learning curve increasing on the validation data? \\n  Do you have enough data to train this classifier? Enlarge the amount of training data and pursue how the learning curves change. How do the curves differ from a more difficult data record? \\n  How do you recognize an overfitting? Does overfitting result in large amounts of data or for small ones? \\n  If you increase the noise of the data record: How do the learning curves change?',\n",
       " 'Evaluation of classifiers\\n\\nIn this notebook we treat the performance characterization of classifiers. You will learn how to use [Scikit Learn] (https: Scikit Learn.org Stable) use metrics, cross -validation and learning curves to determine the generalization of classifiers.\\nIt is important to me that you know the methodically correct procedure with which you can recognize the overfitting danger and protect yourself against it.\\nThe following topics should already be known:\\n The instance of a Scikit Learn Estimator (classifier)\\n How the methods .Fit and .predict are used by estimators to create a prediction.\\n\\nFor repetition, and because it is so important that it cannot be repeated enough often: Machine learning (ML) is about the generalization ability, i.e. the recognition of patterns on examples that have not yet been seen. ML procedures have a tendency to easily learn the training data. Classifiers can easily achieve 100 accuracy on the training data, but that is worthless. One speaks of overfitting when the prediction performance of the classifier on the training data set is greater than on (never seen) test data.\\n\\nThe overfitting term is sometimes also defined more strictly: Abu Mostafa, for example, defines it based on the complexity of the classifier (VC dimension) and says that overfitting only takes place when the following happens: With the increasing complexity of the classification (e.g. depth of a decision tree), validation accuracy takes away. See https: work.caltech.edu telecourse.html.',\n",
       " 'The choice of hyper parameters like K at K NN has still been quite unsystematic. We would like to have a method of how we can use a hyperparameter for any classifier. \\n Parameter curves are happy to be used for this: You train the classifier for a number of hyperparameter values and plot the validation accuracy against the parameter value. It is important in turn to look at the validation accuracy. The validation curve then shows us a suitable value for the hyperparameter that the place on the X axis where the curve reaches its maximum.',\n",
       " 'Validation test data',\n",
       " 'exactly',\n",
       " 'simulator',\n",
       " 'Total function during learning',\n",
       " 'Window format',\n",
       " 'User features read',\n",
       " 'Model reading',\n",
       " 'Window feature',\n",
       " 'Content features Read',\n",
       " 'Column settings',\n",
       " 'State update function during learning',\n",
       " 'XGBoost Predictor',\n",
       " 'Infer',\n",
       " 'Twitter push theme extraction and emotional analysis based on the LDA theme model and emotional dictionary',\n",
       " 'limitation: \\n  In terms of emotional dictionary, the word score is directly obtained by related words, and the influence of the degree of adverbs is not considered.',\n",
       " 'Second, text pre -processing',\n",
       " 'Bags of Words \\n  Perform each document as a word frequency vector, so that text information is converted into digital information that is easy to model \\n  The method of word bag does not consider the order between words and words, which simplifies the complexity of the problem, and also provides an opportunity for the improvement of the model',\n",
       " 'Working environment: (Jupyter Notebook) \\n\\n\\n python 3.7.5 \\n\\n pandas 1.0.1 (processing form data) \\n\\n WordCloud 1.8.1 (Ci Cloud) \\n\\n STYLECloud 0.5.1 (Ci Cloud) \\n\\n NLTK 3.5 (contains various discontinued words, as well as method of stem extraction, word -shaped restoration, and words) \\n\\n Gensim 3.8.3 (contains LDA model) \\n\\n pyldavis 3.2.2 (LDA model visualization tool) \\n\\n NUMPY 1.17.4 (Mathematics Tools) \\n\\n Matplotlib 3.1.1 (data visualization drawing tool) \\n\\n Seaborn 0.11.0 \\n\\n Pillow 7.1.2 \\n\\n TQDM 4.46.0',\n",
       " 'What is the LDA theme model? \\n\\n In LDA, all documents have the same topic set, but each document shows corresponding topics in different proportion. The main goal of LDA is to automatically discover the topic in a document collection. These documents themselves can be observed, and the structure of the topic -topic distribution of topics, topics of each document, and topic assignment of each word of each document -hidden (which can be called Hidden Structure). The core calculation of the topic modeling is to use the observed documents to infer the hidden topic structure. This can also be regarded as the reverse process of generating process -what kind of hidden structure can produce observed documentation collections? \\n\\n\\n  LDA model training \\n\\n There are 5 topic default settings \\n\\n It is expected that the more uniform distribution of TOPIC in the four quadrants, which means that the more comprehensive coverage \\n\\n If the topic is too far away, increase the number of topic \\n\\n If there is a large overlap between different topic, the number of topic is reduced',\n",
       " 'advantage: \\n  Compared with the method of using machine learning and deep learning, using emotional dictionary calculation text scores, no need to be trained and tuned by models, the process is more intuitive, and it is more suitable for the analysis of short text like Twitter tweets (short text texts (short text texts There are usually many sentences that are incomplete and do not conform to the rules of the grammar)',\n",
       " 'LDA visualization \\n  The meaning of each theme (the word on the right is a word that often appears under a theme) \\n  The proportion of each theme in the general language library \\n  Anecdotes between themes',\n",
       " 'Data distribution situation: The amount of data of 2020 3 and 2021 1 is small, and the amount of data in other months is even uniform',\n",
       " 'Ci Cloud Analysis',\n",
       " 'Emotional dictionary calculation text emotional score \\n\\n 1. Determine the negative word dictionary, degree adverbing dictionary \\n\\n 2. Determine whether there is a negative word or degree of adverbs before each emotional word, and divide its previous negative words and degree adverbs into a group. At the degree of the degree of the degree, the score of all groups was added to the positive direction greater than 0, and the less than 0 was negative \\n\\n 3. Analysis of data statistical results',\n",
       " \"process\\n1. Data acquisition:\\n\\nCrazy Twitter data with Python reptile, timeline (2020.3.12 2021.1.12)\\n\\n2. Data cleaning:\\n\\n ① Clear data entry that exists or lack of release time\\n \\n ② Sort the data in order before and after the release time\\n \\n ③ Preliminary analysis of data distribution (with the time of release)\\n\\n3. Text pre -processing:\\n\\n ① Twitter Data ['Content'] initial data (matrix size :)\\n \\n ② PROCESSED DATA removes punctuation,@, short words (matrix size :)\\n \\n ③ Data Words Remove the deactivated word and customized word list (such as search keywords)\\n\\n\\n4. Emotional analysis:\\n\\n ① The keywords are extracted based on the TF IDF algorithm and the word cloud display (overall, monthly)\\n \\n 1. Extract the global TOP20 keywords and generate the word cloud display\\n \\n 2. Segment the global keyword matrix month by month\\n \\n 3. Extract the TOP10 keywords month by month, and generate a word cloud display\\n \\n ② Use the LDA theme model for training and extract the theme distribution\\n \\n 1. Build the LDA model and train\\n\\n 2. Calculate the model of the model (Perplexity) and the model consistency score.\\n\\n 3. For the topic of the distribution of each tweet, the theme (high probability)\\n\\n 4. Adjust other parameters such as the number of themes or the number of training iterations, iterative adjustment\\n \\n ③ Use the emotional dictionary method to analyze the text, and calculate the text emotional score\\n \\n 1. Emotional Dictionary uses SENTIWORDNET3.0, negatives used dictionary, and degree of admittance dictionary\\n \\n 2. Determine whether there is a negative word or degree of adverbs before each emotional word, and divide its previous negative words and degree adverbs into a group. At the degree of the degree of the degree, the score of all groups was added to the positive direction greater than 0, and the less than 0 was negative\\n \\n 3. Calculate the text score based on the emotional dictionary, each tweet will calculate a score. The more positive its emotions, the higher the score; the more negative, the lower the score\\n \\n 4. Analysis of data statistical results\\n\\n5. Results analysis\",\n",
       " '1. Data cleaning',\n",
       " 'Initialization parameter settings \\n  Global selection keyword top n \\n  Select keywords TOP N month by month \\n  Selection of themes \\n  Number of training iterations',\n",
       " 'Observe the distribution of the original data at 2020.3.12 2021.1.12',\n",
       " 'Conceptual explanation \\n \\n \\n  What is LDA? \\n\\n  LDA (hidden Delikrey distribution) gives the theme of each document concentrated in the documentation in the form of probability distribution \\n\\n\\n\\n  What is TF IDF? \\n\\n  TF: Term Frequency (For a certain text, the more important the words appear in a certain text) \\n\\n  IDF: Inverse Document Frequency \\n\\n  TF IDF is a calculation method that measures the importance of a document in a document. Through the TF IDF formula, we can calculate how the word contributes to the theme of this document. The main idea of TFIDF is: if a word or phrase appears in an article, the frequency TF is high and rarely appears in other articles. Or extract keywords. \\n\\n\\n\\n\\n  What is an emotional dictionary?',\n",
       " 'The difference between TF IDF (keyword weight) and LDA (word distribution) \\n\\n  The limitation of using TF IDF for keyword extraction is that in some scenarios, the keyword extraction based on the document itself is not very sufficient. Some keywords may not be displayed in the document. Popular science, introducing lion tigers, etc., but the word animal two is not displayed in the article. \\n\\n  The LDA theme model believes that there is no direct connection between words and documents. They should have a dimension to connect them in series. The theme model refers to this dimension as the theme. Each document should correspond to one or more themes, and each theme will have a corresponding word distribution. Through the theme, you can get the word distribution of each document.',\n",
       " 'Based on the TF IDF algorithm generate cloud cloud',\n",
       " 'Data prep',\n",
       " 'Data Validations\\n do all categories in train exist in test?',\n",
       " 'Importing Required Libraries',\n",
       " 'Data Visualization',\n",
       " '2.1 Import data',\n",
       " '3. Prepare data \\n Prepare the data for the analysis. What can go away, what is broken and which features can I still extract?',\n",
       " 'S 1: ... \\n\\n Vomiting: ... \\n\\n ...',\n",
       " '2.2 View data and scheme',\n",
       " '5.2 Make a decision',\n",
       " '4.2 Interpret the results',\n",
       " 'Your team: \\n Please enter your names here and shared the document with me. Please do not make your own document open \\n  Name1 \\n  Name2 \\n  ...',\n",
       " '3.1 Clean data',\n",
       " 'Your task: \\n A US American real estate mogul, David Dromp, wants to expand his fortune by investing in real estate in New York. Since he is no longer considered to be accountable due to past decisions, you were commissioned to support the investment decision with data. \\n\\n Your data record contains sales data from real estate in New York for the past year. You should examine which types of real estate have achieved particularly high sales and which types of real estate have gained great value last year. Based on this, you should make a data -driven decision in what kind of real estate the billions of real estate mogul should flow. \\n\\n Further information on your data record can be found here: \\n https: www.kaggle.com New York City NYC Property Sales',\n",
       " '2. Explorize data \\n First of all, import the data and prepare for modeling. \\n Take a look at what the data looks like, how many there are and what features there are.',\n",
       " '3.2 Extract features',\n",
       " '2.3 Examine data (sample)',\n",
       " '1. Ask questions \\n What questions do I have? What do I want to find out using the data?',\n",
       " '5.1 Discuss results',\n",
       " '5. Communicate results \\n You visualized and interpreted the data. Now discuss your results and lead your decision. You may want to represent an important plot from 4 again.',\n",
       " 'Submission Template \\n This is a template for your delivery at the end of the course. You can use it or change it as you want or create your own. In the end, it is important to have a stringent and sensible structure in the notebook.',\n",
       " '0.0 Imports',\n",
       " '4. Analyze data \\n Analyzes the data. Is there anything interesting to see? Also keep examples of plots that have brought nothing (this is also a realization). If you notice that the data does not fit, then take a look at point 3.',\n",
       " '2.4 Examine data (statistical)',\n",
       " '4.1 Create graphs',\n",
       " 'The dataset was anonymized by removing some outliers that could identify the interviewee and therefore not all data collected in the survey will be available here. States with lower incidence of response, such as those from the North, Northeast and Midwest, will only have their region indicated in the Dataset, also as a consequence of the anonymization process.',\n",
       " 'Creation of datasets',\n",
       " 'Convolutive networks',\n",
       " 'Exercice Face Mask',\n",
       " 'Linear regression',\n",
       " 'XGB regressor',\n",
       " 'Classification',\n",
       " 'KNN Pipeline',\n",
       " 'Preprocessing del train set',\n",
       " 'Outliers removal',\n",
       " 'Scaling PCA',\n",
       " 'Missing price processing',\n",
       " 'Model using LightGbm',\n",
       " 'Creating learning data and evaluation data',\n",
       " 'Replace SEX, Embarked and PClass',\n",
       " 'Predict age',\n",
       " 'Unused columns',\n",
       " 'Category data conversion',\n",
       " 'Model creation',\n",
       " 'End of pre -treatment',\n",
       " 'CABIN (room information (such as sea side))',\n",
       " \"Passengerid doesn't seem to be necessary \\n  PCLASS: Passenger class (1 = 1st, 2 = 2, 3 = 3 class). It is a guide to wealth \\n  It may be better to treat it as a category data \\n  SIBSP: Number of brothers and spouses (spouses) on the Titanic \\n  PARCH: Number of parents and children (Children) on the Titanic\",\n",
       " 'evaluate',\n",
       " 'Two Embarked of Train with a defect are not added to learning \\n  Replaced Test Fare with a deficient value by the average of Train',\n",
       " 'Further investigate Ticket',\n",
       " 'Creating Dataset',\n",
       " 'Model using NN',\n",
       " 'Extract the title from Name',\n",
       " 'Test is age cabin \\n  Train is age cabin embarked \\n  Name Sex Ticket Cabin Embarked is not a numerical value',\n",
       " 'Ensemble learning \\n\\n 1. Model using LightGbm \\n 1. Model using NN',\n",
       " 'NN model creation procedure \\n 1. Make a dataloader \\n  1. Create learning data and evaluation data \\n  1. Create a dataset (converted into a type used in pytorch) \\n 1. Make a model \\n 1. Learn \\n 1. Evaluate \\n 1. Adapt to test data',\n",
       " 'Data reading',\n",
       " 'Use LightGbm',\n",
       " 'Learning, creation of evaluation data',\n",
       " 'Homework \\n\\n 1. Find out how many of the test subjects regularly have breakfast \\n 2. How tired do the subjects feel on average the breakfast? \\n 3. What do you think: Which factors are crucial that you feel fit during the day? Attempt to justify this with the information given reduces the problem to the features Hours and Breakfast',\n",
       " 'The individual steps for cells for a complete diagram are expanded below. This actually happens in a cell, but I should better understand the individual steps. \\n\\n  New steps are marked with one.',\n",
       " 'exercise 2 \\n\\n 1) \\n Create a simple plot that illustrates the wage development over the years of managers inside (F and M) \\n\\n 2) \\n Create another plot that represents the wage differences between female and male. For this we want the wage difference for managers and for machine operator on the inside in a plot \\n\\n  Make sure that your plots contain all important information: \\n  title \\n  Axis lettering \\n  Legend and label',\n",
       " 'Homework \\n\\n  Find 5 professional groups \\n  Represent the wage difference for 2016 in a column diagram',\n",
       " 'Paygap data set',\n",
       " 'Matplotlib \\n  Data visualization',\n",
       " 'The data record is far too small to make a reliable statement based on the attributes. \\n\\n Does not simply construct a prediction without checking the reliability of your statement.',\n",
       " 'Different plot types',\n",
       " 'Task 1 \\n\\n 1) \\n  Open the Kaggle Input CusersmarildownloadSearningcsv earning.csv data set. When opening you have to set the delimiter differently \\n  First look at the data record in peace. Also use the info () radio to give you an overview \\n\\n 2) \\n The data record contains a lot of features. At first we only want to look at a few selected people. To do this, form a new DataFrame, which now contains the following columns from the origin of the data fan: \\n  year \\n  Femalesmanager \\n  male manager \\n  Femalemachinery operator sand drivers \\n  Malemachinery operator sand drivers',\n",
       " 'Proportion of healthy vs. healthy products',\n",
       " 'Nuts products cover much of the market, and offer a wide range of price. \\n Dietary and healthy cookie products occupy small part of the market, and their costs are a bit higher than that of other products.',\n",
       " 'Top 7 companies with more healthy products in the market',\n",
       " 'Amount of products offered depending on its category (healthy unhealthy) and its brand.',\n",
       " 'Objective: \\n Perform an exploratory analysis of meals, prices between healthy products vs.',\n",
       " 'Analysis performed with the average cost of all products. It would be ideal to make an approximation using the average cost per gram',\n",
       " 'In graph it can be evidenced that unhealthy products lead the list, the main fries and packages being the main ones.',\n",
       " 'Top 7 companies with more unhealthy products in the market',\n",
       " 'Of healthy campaigns which is the proportion of unhealthy products',\n",
       " 'With the execution we wanted to answer the question How healthy are the companies that sell healthy products? As insights we find that the Frito Lay company with a striking name for unhealthy products has in its inventory a greater amount of healthy products , additionally it was found that from the top of healthy companies it is the only one that has some unhealthy products. \\n\\n On the other hand, the JBO MP company has non -healthy products in its inventory although to a lesser extent to the healthy ones.',\n",
       " 'III. Preprocessing',\n",
       " 'Why detection of fraud? \\n  Fraud is a billion dollars and it increases each year. The 2018 economic crime PWC global survey revealed that half (49) of the 7,200 companies interviewed had been victims of any fraud. This is an increase compared to the 2016 PWC study, in which just over a third of the organizations interviewed (36) had been victims of economic crime. \\n\\n\\n This competition is a problem of binary classification, that is to say that our target variable is a binary attribute (the user who clicks is it fraudulent or not?) And our goal is to classify users as fraudulent or non -fraudulent Better possible.',\n",
       " 'Load data',\n",
       " 'Team \\n Mohamed NIANG \\n Fernanda Tchouacheu \\n Sokhna Penda Toure \\n Hypolite Chokki',\n",
       " 'Table of Contents \\n\\n I. Introduction\\n\\n II. Descriptive Statistics Visualization\\n\\n III. Preprocessing\\n\\n IV. Machine Learning Models',\n",
       " 'Pipeline of preprocessing',\n",
       " '',\n",
       " 'I. Introduction',\n",
       " 'Merge transaction identity',\n",
       " 'DataCamp IEEE Fraud Detection',\n",
       " 'Method 2',\n",
       " 'Method 1',\n",
       " 'Work type 1 example problem \\n It is a problem of asking the number of records (ROW) with a value that is more than 0.5 after converting the QSEC column to Min Max Scale in the car data set. \\n  Data Source: https: www.kaggle.com ruiromanini mtcars \\n  How to add data: right menu add data mtcar (Ruiromanini) Add \\n  Video link: https: YouTu.be e86QFVXPM5Q',\n",
       " '4 Fill in the column fields with categorical values with the fashion of column values, so use the Fillna () and Fashion () function () function \\n  Define a vector with the columns to be changed \\n  Use a loop to sweep the vector (for n in columns: df.fillna ...)',\n",
       " '2 perform the function describes to see a descriptive statistical summary',\n",
       " '1. Title: Credit Approval\\n2. Sources: \\n (confidential)\\n Submitted by quinlan@cs.su.oz.au\\n3. Past Usage:\\n See Quinlan,\\n Simplifying decision trees , Int J Man Machine Studies 27, Dec 1987, pp. 221 234.\\n C4.5: Programs for Machine Learning , Morgan Kaufmann, Oct 1992\\n4. Relevant Information:\\n This file concerns credit card applications. All attribute names\\n and values have been changed to meaningless symbols to protect\\n confidentiality of the data.\\n This dataset is interesting because there is a good mix of\\n attributes continuous, nominal with small numbers of\\n values, and nominal with larger numbers of values. There\\n are also a few missing values.\\n5. Number of Instances: 690\\n6. Number of Attributes: 15 class attribute\\n7. Attribute Information:\\n A1:\\tb, a.\\n A2:\\tcontinuous.\\n A3:\\tcontinuous.\\n A4:\\tu, y, l, t.\\n A5:\\tg, p, gg.\\n A6:\\tc, d, cc, i, j, k, m, r, q, w, x, e, aa, ff.\\n A7:\\tv, h, bb, j, n, z, dd, ff, o.\\n A8:\\tcontinuous.\\n A9:\\tt, f.\\n A10:\\tt, f.\\n A11:\\tcontinuous.\\n A12:\\tt, f.\\n A13:\\tg, p, s.\\n A14:\\tcontinuous.\\n A15:\\tcontinuous.\\n A16: , (class attribute)\\n8. Missing Attribute Values:\\n 37 cases (5 ) have one or more missing values. The missing\\n values from particular attributes are:\\n A1: 12 A2: 12 A4: 6 A5: 6 A6: 9 A7: 9 A14: 13\\n9. Class Distribution\\n : 307 (44.5 ) : 383 (55.5 )',\n",
       " 'Categorical columns: A1, A4, A5, A6, A7, A9, A10, A12, A13 \\n  Categorical columns with missing values: A1, A4, A5, A6, A7 Fill in fashion (value that appears most in the column) \\n  Columns with continuous values: A2, A3, A8, A11, A14, A15 \\n  Columns with continuous values with missing values: A2 and A14 fill with average',\n",
       " '6 Transform all columns with categorical values for the category type use the Astype function \\n  Categorical columns: A1, A4, A5, A6, A7, A9, A10, A12, A13',\n",
       " '7 Transform Objective A16 column first to the category type and then numeric coding using the DF Function [A16] .cat.codes',\n",
       " '5 Fill in the missing columns with continuous values with the average \\n  Columns with continuous values with missing values: A2 and A14',\n",
       " '11 Run the decision tree algorithm, keep the result of prediction in a new predictad column',\n",
       " \"8 Use the PD.get dummies function to transform all categorical columns for variable indicators. \\n  DF = Pd.get Dummies (DF, Columns = ['A1', 'A4', 'A5', 'A6', 'A7', 'A9', 'A10', 'A12', 'A13'])\",\n",
       " \"1 Import File (Credit Screening.Data) Look for DataSet in Kaggle, if you can't find, import the file available on Google Classroom. Import the file replacing missing values? by nan and insert the name of the columns using the vector below. \\n\\n  Names = [A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16] 5\",\n",
       " '10 Run the nearest neighbor K = 3 neighbors algorithm, store the result of the prediction in a new column of Dataframe Predictknn',\n",
       " '3 Use DF.Isnull (). Sum () to know which columns there are missing values nan',\n",
       " '9 Use Train Test Split to create training and testing bases',\n",
       " '(8)',\n",
       " '(9)',\n",
       " 'We can see that there are no missing values in the data.',\n",
       " '(10)',\n",
       " 'We are looking for the most correlated variables to improve the effectiveness of the cluster.',\n",
       " '(7)',\n",
       " '(12)',\n",
       " '(11)',\n",
       " '(13)',\n",
       " '(6)',\n",
       " 'Next we do some simple data analysis \\n 1. The distribution of hotels in different districts \\n 2. Different volume of empty housing in different blocks \\n 3. List the Hoster with more than 200 hotels in New York City \\n 4. List the hotel in New York City with a price of $ 200',\n",
       " 'Below we use the coordinating party to preliminary observation of those in the data that may exist in correlation',\n",
       " '4.1 Related coefficients and covariances',\n",
       " \"For NAN's data, we can get it by assigning value\",\n",
       " 'Correct two misunderstandings, which can be used on DataFrame and Series for ILOC and LOC. Series using index (key), which contains the last one, so using the Loc () to use the slice containing the last data. But using iloc () does not include the last one)',\n",
       " '3.1 Re -index',\n",
       " 'In the Series, using the Corr calculating table, overlap, non -nan, according to the correlation coefficient of the index. Similar to this, COV is used to calculate the covariance difference',\n",
       " 'We know that Series is an indexed dictionary, so if we import the dictionary, then we can enter the value, or we can determine the key at the same time',\n",
       " 'Series can be regarded as tables.',\n",
       " 'The element can be located through the call and column call',\n",
       " 'For Series',\n",
       " '1.1 He is a list with indexes',\n",
       " 'Dataframe can create a form, \\n The file loaded in this form is a dictionary, \\n The header of the table is the key of the dictionary, \\n The data of the form is the value of the dictionary, \\n The value of the table is an array form',\n",
       " '3.4 Use LOC and ILOC to select',\n",
       " 'The two categories of Series and DataFrame have changed the indexes in the case of determined conditions.',\n",
       " 'In the above question, we will add and subtract the two forms, and there will be many nANs. We need one way to eliminate nan',\n",
       " 'Use the Pands DataReader module to download the stock data',\n",
       " '1. Understand the server',\n",
       " 'For Series',\n",
       " '3.7 Fill in the calculation method',\n",
       " \"Another form of data is the nested of multiple dictionaries (usually nests of two dictionaries). Pandas's analysis is: the outermost key is the head of the head, and the key inside is the head.\",\n",
       " 'First of all, the operations of the two different lists are broadcast. If one table is the same as the index of the other table, its addition and subtraction must be calculated',\n",
       " \"For Series, you can use Values' method\",\n",
       " 'Vertical',\n",
       " \"For each element of pands, I can use several methods to calculate: 1. Use Numpy's element -level method; 2. Python's function or anonymous function\",\n",
       " '3.10 sorting and ranking',\n",
       " 'Our digital operations of Serise are all needle elements and do not change the index',\n",
       " '4.2 Unique value, count value and member qualifications',\n",
       " '3. Pands basic function',\n",
       " '3.5 integer index',\n",
       " '3.8 operations between DataFrame and Series',\n",
       " 'Columns and index represent the heads of each column and each line, respectively, which can be changed',\n",
       " '2 DataFrame understanding',\n",
       " 'Isin determines whether it is the value in the table',\n",
       " 'Use function',\n",
       " 'How to explain this problem: The elements of the Series class we use, so its indexing method is different from the Python built -in dictionary and list, so it cannot be parsed when the serial number is 1, but when the index is set, the 1 serial number can be introduced.',\n",
       " 'Specific explanation https: blog.csdn.net code porter article defics 86517233',\n",
       " '4. Calculation description statistics',\n",
       " 'The ranking uses built -in Rank ()',\n",
       " 'For dataframe',\n",
       " 'The operation of the index is: determine whether this index is in this Series',\n",
       " 'The index here can be changed by changing the index',\n",
       " 'This method can be determined by the parameter file value =, which will make the final number nan because of the lack of indexes',\n",
       " 'You can get the required number by the choice of indexes',\n",
       " 'We change the index and add the key without Value, then this key corresponds to Nan. In the same way, if we delete an index with Value, this index and value will be deleted',\n",
       " 'For Series, about the use of the key, then the end of this slice is included. \\n If it is sliced, it is needed, it will not be included in the last one',\n",
       " '3.6 calculation operation and data alignment',\n",
       " 'In summary, for the data of Series and DataFrame, the addition of calculations (note that add here), there are the same elements to add it up; there is no element that will be displayed (as long as one of them is not shown) , Value becomes nan',\n",
       " '3.2 Discard the items on the specified axis',\n",
       " 'Series and DataFrame are different tables, and there is a need to delete a column or row of the form. At this time, we need to use the Drop () method.',\n",
       " 'Loc and ILOC are data selected by data under DataFrame and Series. LOC is the key value, and ILOC is the serial number represented by',\n",
       " \"Use Pandas's isnull and notnull function to detect the actual data\",\n",
       " 'Change the index of two directions: one is to increase the index, and the other is to delete the index. If it is deleted, the data corresponding to this index disappears. If it is an increased index, then NAN data will be generated. So when we increase, we use Method and some methods to fill the data as much as possible.',\n",
       " '3.9 Application and mapping of functions',\n",
       " 'For a form, we need to judge whether the element is unique without knowing the full appearance. If it is not unique, we have to count the number of repetitions',\n",
       " 'If we do an operation for this data, then all data of this data is performed (excluding index)',\n",
       " '3.3 Costen, selected and filter',\n",
       " 'For DataFrame, if you want to sort specific columns, then use BY parameters to specify',\n",
       " \"Let's talk about sorting first, sorting is to sort from small or large through indexes and data\",\n",
       " 'For dataframe',\n",
       " 'Horizontal',\n",
       " 'To put it plainly, the purpose is that there is a rule to draw the ranking of the data',\n",
       " 'Get data set \\n\\n Read the file specified by the parameter and return an object of the DataFrame type. \\n\\n There are two data sets: training sets and test sets. We will use the training set to build our prediction model, use the test set to score it and generate output files to submit on the Kaggle evaluation system.',\n",
       " 'As can be seen \\n  Baby (Age = 4) has a high survival rate \\n  Old man (Age = 80) all survive \\n  A large number of 15 25 passengers did not survive \\n  Passengers are mainly within the age of 15 35 \\n\\n  1.3 Type SEX and numerical type SurVived (here is a numerical Survived, which may need to be placed in other positions)',\n",
       " 'Obviously, we can see that PClass = 1 passenger survival rate is higher \\n\\n  2.3 The relationship between category PCLASS, numerical AGE, category SurVived \\n\\n ASPECT: The horizontal axis length and vertical axis of each small chart, defaults to 1 height: the height setting of each small chart, default 3',\n",
       " 'Point of knowledge\\n\\nDisplot () gathers the function of the Hist () of Matplotlib and the nuclear function estimated KDeplot, adding a novel purpose of the Rugplot distribution observation bar display and using the SCIPY library FIT fit parameter distribution. The specific usage is as follows ::\\n\\n \\nSeaborn.distPlot (a, bins = none, history = true, kde = true, rug = false, fIT = None, history kws = none, kde kws, rug kws = none, fit kws = none, color = noNe, color vertical = false, norm history = false, axlabel = none, label = none, ax = none)\\n \\n Parameter AX: Select location\\n Parameter Hist, KDE, regulate whether the histogram and nuclear density estimation (default HIST, KDE are all)\\n\\n \\n\\nQuestion: The parameter BINS in the histogram, what is it based on?\\n\\n Extreme situation, bins = 1\\nThe above figure shows extreme situations. When bins = 1, all data indicates that all data are in a group. Too rough grouping affects the obvious nature of the data packet, and nothing can be seen.\\n\\n Extreme situation, bins = data size\\nIn the figure below, comparison of extreme situations, when bins = len (data), indicate that if all data is different, they will appear the same group as the data length. Too detailed grouping will cause a large error, so the determination of the number of groups should be appropriate.\\n\\nThe width of the group spacing: the number of groups in the data (maximum value and minimum value)',\n",
       " 'Knowledge point: [LOC according to conditions, assignment of new listing] (https: blog.csdn.net QQ 36523839 Article Details 80502574) \\n\\n  df.loc [condition, new list] = the initial value \\n\\n If the new list is named, it will change on the original data column',\n",
       " 'Assume \\n\\n  Which characteristics may be related to survival?',\n",
       " \"Explorest data analysis\\n\\nBegan to explore data and understand the data with problems.\\n\\nLearn data through the following attributes and methods\\n Attributes\\n df.columns.Value data set all columns (features), numpy.ndarray type\\n method\\n df.head () Preview of the front 5 lines of the data set\\n df.tail () Preview 5 lines after the data set\\n DF.info () shows general data information, including the name of each column name, the number of non -vacant values, the data types of each column, memory occupation and other information.\\n DF.Descrip () show some descriptive statistical information of the data, but it will filter out the missing value. The field content of the numerical type is only calculated by default.\\n description (include = ‘all‘), counting all types of data\\n Descripe (Include = [np.number]) Only count the field content of the numerical type: count count, MEAN average, STD difference, minimum min, quarter, MAX maximum value\\n Descripe (Include = [NP.Object]) Only count the field content of Object type\\n Describe (Include = 'O') Only count the field content of the string type: count count, unique unique value, the highest frequency of TOP, the highest frequency of FREQ appears frequency\\n Descripe (Percentiles = []): Set out the percentage of output, default [.25, .5, .75] and returned 25, 50 and 75 percentage.\\n\\n Preview data\\n\\n df.head () Preview of the front 5 lines of the data set\",\n",
       " \"Treatment characteristics Cabin creates new features: deck number (this method is not used for the time being) \\n\\n I wanted to delete the feature of 'cabin' directly, but later I discovered some interesting things. The cabin number 'C123', which looks like the letter represents the deck number. Therefore, we want to extract these and create a new feature, which contains a person’s deck number \\n\\n  Fillna fills the missing value \\n In PANDAS, the missing data generally uses NAN marked NOT A Number. In particular, in the time sequence, the loss of the timestamp is nat mark.\",\n",
       " \"According to gender classification, Embarked seems to be related to survival. \\n\\n  No matter which port boarding the ship, under the premise of PClass, the survival rate of female passengers is higher than that of men. \\n  For male passengers boarding from S or C ports, the survival rate of PClass = 1 is higher than that of PCLASS = 2, PClass = 3. \\n  Among the male passengers boarding from the Q port, the survival rate of PCLASS = 3 is higher than that of PClass = 1, PClass = 2. \\n\\n PClass seems to be related to the survival rate. Let's observe the relationship between PClass and Survived \\n\\n  2.2 Type PCLASS, Numerous SurVived relationship\",\n",
       " 'Creating a title feature can also be used to better estimate the age that is missing. \\n\\n  Processing feature AGE missing value fill in \\n\\n It is not simply filled the missing Age value with the average age or median age of the data set, but through the gender, level and grouping of passengers, we can understand the age of passengers more deeply. \\n\\n We will group data sets according to gender, PCLASS (passenger level) and Title, and calculate the median age for each subset. \\n\\n In order to avoid the data leakage of the test concentration, the age of the test concentration is filled with the value of the data leakage from the training set.',\n",
       " 'So we extracted the name information. Because some people who are also titles are too small, we need to make a mapping \\n\\n  MME: Women who call the upper society of non -English -speaking nation, and professional women, equivalent to MRS \\n  Jonkheer: Gentlemen \\n  Capt: Captain? Essence Essence \\n  Lady: The name of the noble lady \\n  DON: The name of the aristocracy in the Spanish and the status \\n  SIR: Everyone understands \\n  the countes: Earl Earl \\n  MS: ms. Or MZ, the United States recently used to call women with unknown marriage status \\n  Col: Lieutenant: Lieutenant Colonel (lt. Col.) Colonel: Colonel (color (color)) \\n  Major: Major \\n  Mlle: Miss \\n  Rev: Rev.',\n",
       " 'It can be seen that the survival rate of female passengers is higher than men. \\n\\n  2.1 Category Embarked, Category PClass, Category SEX, Numerous Survision 4',\n",
       " \"Seeking the missing value of a certain column \\n\\n Because the method of selecting a certain column in the DataFrame data is (according to the typical marking or attributes as Series) \\n  DF.A attribute method \\n  DF ['a'] method \\n So there are two types of missing values in a certain column \\n  df.a.isnull (). Sum () \\n  df ['a']. isnull (). Sum () \\n\\n Depending on a column to remove the missing value \\n  df.a.isnull (). DropNA () \\n  df ['a']. isnull (). Dropna ()\",\n",
       " 'Knowledge point: regular expression formula\\n\\n ([A ZA Z]) This indicates that at least one or more large -writing letters are matched with a group\\n . Matching any characters other than changing the row\\n Match the characters in front of the characters once or multiple times (= 1)\\n [A ZA Z] You can match a letter regardless of the case\\n\\n Re.Compile (Pattern, Flags = 0)\\nThis method can compile the regular string into a positive expression object (Pattern object) in order to reuse it in the later matching\\n \\n prog = re.comPile (pattern)\\n result = prog.search (string)\\n \\n Equivalent to\\n result = re.Search (pattern, string)\\n\\n Group\\nYou can use the Group (NUM) or Groups () matching object functions to obtain matching expressions.\\n Group (num = 0) The entire expression string matching\\n Group () can enter multiple group numbers at a time, and in this case, it will return a fiber group containing the corresponding values \\u200b\\u200bof those groups.\\n Groups () returns a group group containing all group string, from 1 to the group number contained.\\n \\n Anonymous function\\n lambda x: x x\\nThe keyword Lambda represents anonymous function, and the X in front of the colon indicates the function parameter. Equivalent to\\n \\n def f (x):\\n Return x x\\n \\n\\n Mapping\\n Series data\\n S.Map () all transmit the corresponding data as a parameter into the dictionary or function, and get the value after mapping\\n Use dictionary mapping\\n Use the function to mappore\\n The principle of S.Apply () is similar to the MAP method. The difference is that Apply can pass the function with more complicated functions (the function of the MAP can only receive one parameter)\\n Dataframe data (default axis = 1) (axis = 1 is horizontal, Axis = 0 is vertical)\\n df.apply () Apply the function to each line or each column\\n Axis = 1 Pass the data of each line of data in the form of a Series (index of Series) into the specified function\\n Axis = 0 The column (columns) is passed in the form of a Series as a parameter to the operation function you specified\\n df.applymap () Apply a function to each element\\n \\n ASTYPE (int)\\n For data types for transforming Dateframe a column\\n\\n Processing other features Embarked, Fare Lost value filling\\n\\n The training concentrated EMBARKED feature has only two missing values, filling with the number of crowds.\\n The training concentration Fare feature is only 1 missing value, which is filled with median.\\n The training concentrated Cabin features 687 missing values, filled with constant ‘U’ (UNKNOW).\\n\\n Crowd\\n df.fillna (df.value counts (). Index [0])\\n df.fillna (df.mode (). ILOC [0])\\n\\n Medium number filling\\n df.fillna (df.median ())',\n",
       " '2.4 The relationship between numerical Sibsp, numerical PARCH, numerical SurVived 3 \\n\\n The combination of Sibsp and Parch is more meaningful to use together, and the combination shows the number of relatives with passengers. And calculate the number of people who travel alone and non -separate travel',\n",
       " \"coding \\n\\n Before starting modeling, the last step we want to execute is to convert all of our classification characteristics to numbers, because our algorithm can only accept one number array as input, not names or letters. We have a few columns to be converted. We use Pandas's PD.GET DUMMIES () method to convert the classification features into digital features. \\n\\n Label Encoding, One Hot Encoder encoding \\n [Categorical Encoding USING Label Encoding and One Hot Encoder] (https: TowardsDataScience.com Categorical Encoding and ONE HOT Encoder 911ef77fb5bd)\",\n",
       " 'The average value of Survied is 0.383838, while the Survied value is only 0 or 1, indicating that this average reflects the survival rate.\\n The survival rate of the sample is about 38, and the actual survival rate is 32 (2224 1502 2224 = 32.4)\\n \\nFrom the current point of view, pay attention to a few points.\\n\\n Non -value -type features need to be converted to numerical features so that machine learning algorithms are processed later.\\n The range of these characteristics is very different, and it needs to be converted to approximately the same scale.\\n Some features include missing values \\u200b\\u200b(nan = not numbers), and we need to deal with it.\\n\\n Data set loss loss details\\n\\n DF.ISNULL () Return to indicate which value is a loss -loss Boolean value\\n df.notnull () Return to indicate which value is not a loss -loss Boolean value\\n DF.DROPNA () Filter the axis label according to whether the value of each label is missing, and determine the threshold based on the amount of data allowed to be lost\\n DF.Sort Values \\u200b\\u200b() Sort, default, ascending = false indicates the order\\n PD.Concat ([]) enables the object to bond or \"stack\" on the axial direction, and the default is an axial direction along Axis = 0 (line). Axis = 1 represents axial (column).\\n ROUND (x [, N]) method Return to the four houses and five entry values \\u200b\\u200bof floating -point number x, n decimal points',\n",
       " 'Training data set\\n Number of rows: 891 sample size\\n Number of columns: 12 (11 features 1 target variable SurVived)\\n Data type: 7 features are integer or floating points, and 5 features are string\\n Lack of value: Cabin Age Embarked (quantity from large to small sorting)\\n Numerical data: Passengerid, Age, Fare, Sibsp, Parch\\n Category data: Survived, Sex, Embarked, PClass\\n Hybrid data: TICKET, Cabin\\n\\n Test data set\\n Number: 11 Features\\n Data type: 6 features are integer or floating points, and 5 features are string\\n Lack of values: Cabin Age Fare (quantity from large to small sorting)\\n\\n \\n\\n Point of knowledge\\n Category data (qualitative data)\\n\\n Data are divided into various categories to describe the nature or characteristics of certain categories. Therefore, category data is also called qualitative data. The type of game is an instance of qualitative data -each game type forms an independent category. Regarding qualitative data, please keep in mind a point: you cannot understand the data value as a number.\\n \\n\\n Numerical data (quantitative data)\\n\\n Different value data is different, and it involves numbers. The numerical values \\u200b\\u200bin numerical types have the significance of numbers, but also involved measurement or counting. Because the numerical data describes the quantity, it is also called quantitative data.\\n\\n \\n\\n Description statistics of datasets\\n\\n df.descrip () Returns the data value of the data, the field content of the numerical data: count count, the average MEAN, STD variance, minimum min, quadrilateral, MAX maximum value value',\n",
       " 'Oliva Y OCANA, DONA. Fermina. This has not encountered this title in the training data set.',\n",
       " \"As can be seen\\n\\n For men, the survival rate between about 20 and 35 years old is high; the probability of survival between 5 and 18 years old is very low,\\n For women, the probability of survival between 15 and 40 is higher\\n Regardless of men and women, babies have a higher probability of survival.\\n\\nThere seems to be some specific age groups that increase the chance of survival. Let's observe the relationship between AGE and SurVIDD\\n\\n 1.2 Relationship between Numerous AGE and Category SurVived\\n\\nThis class is very useful when you want the relationship between the distribution of visual variables or multiple variables when the sub -concentration of the dataset can visualize the variables, respectively. A FaceTGrid can be concluded with as many as three dimensions: row, color, and hue. The first two have obvious correspondence with the obtained shaft array, which regards the color variables as the third dimension along the deep shaft, and different levels are drawn with different colors.\\n\\n \\nFACETGRID (data, row = none, color = none, hue = none, color = none, sharex = true, sharey = true, height = 3, aspect = 1, palette = none, row order = none, color = none ,\\xa0hue order=None,\\xa0hue kws=None,\\xa0dropna=True,\\xa0legend out=True,\\xa0despine=True,\\xa0margin titles=False,\\xa0xlim=None,\\xa0ylim=None,\\xa0subplot kws=None,\\xa0gridspec kws=None,\\xa0size=None Cure\\n \\n\\n Variables on the column (left and right)\\n variable on ROW (up and down)\\n\\nHere, the FaceTGrid function is used for the value of different survival rates, and it is separately divided into two histograms.\",\n",
       " 'Treatment features Sibsp and Parch create new features FamilySize',\n",
       " \"Treatment characteristics name Create new features: passenger titles \\n\\n When viewing the name of passengers, people may want to know how to deal with them to extract useful information. If you take a closer look at these first examples: \\n\\n  Braund, Mr. Owen Harris \\n  Heikkinen, miss. Laina \\n  Peter, master. Michael j \\n\\n You will notice that each name has a title! This may be a simple lady (MISS.) Or Mrs., but it may sometimes be as complicated as masters and Sir. In this case, symmetry can be classified. Let's see how we will perform this operation in the following functions. \\n\\n Let's take a look at the differences in the training concentration.\",\n",
       " \"Observed: \\n  The survival rate of one to 3 digits of the same experts is higher \\n  Except for 6 places, the survival rate of only one or more than 3 places in the peers is low. \\n\\n decision making: \\n  Incorporate a model \\n\\n  Characteristic engineering \\n\\n Both the training set and the test set need to be dealt with, so merge it. \\n\\n method \\n  Delete unrelated characteristics \\n  Create a new feature (according to the existing characteristics, digging valid information to create new features) \\n  Lost value \\n  According to the missing values we know before, deal with it one by one \\n  Training data set \\n  Embarked features have only 2 missing values, which can be easily filled. \\n  Age's features are more troublesome because it has 177 missing values. \\n  Cabin features need further investigation, but it may seem to be deleted from the data set, because the proportion of missing values is as high as 77. \\n  Test data set \\n  Fare features only 2 missing values, which can be easily filled. \\n  Age has 86 missing values. \\n  Cabin features need further investigation, but it may seem to be deleted from the data set, because the proportion of missing values is as high as 78.2.\",\n",
       " 'Treatment feature passengerid deletion feature \\n\\n Training concentrated delete feature passengerid because it does not affect the survival rate. At present, the test concentration is not available for concentration of Passengerid, because it is necessary to submit it.',\n",
       " 'Preliminary judgments, except for Passengerid, TICKET, and Name, other characteristics may be related to survival. \\n\\n  1.1 Relationship between the three -value AGE, category SEX, and category SurVived: \\n\\n  First divide the data according to gender (female, male) \\n  Based on gender, and then divide according to the surviving state (Survived, Not Surviving) \\n  There will be four matching \\n  Female Survived \\n  Female Not Survived \\n  Male Survived \\n  Male not survived \\n  In view of the lack of values of AGE features, when visual analysis is available here, the loss of AGE needs to be removed',\n",
       " 'For NAN filling U0, you can see the deck number. The training set is from A to G, and then t, and the test set is from A to G, and there is no splint number that does not exist in the training concentration. \\n\\n We convert this feature into a digital variable. The lack of value will be converted to zero.',\n",
       " \"Knowledge point ASTYPE (int) \\n\\n For data types for transforming Dateframe a column \\n\\n As follows, the STR type of Dateframe's column is changed to int. Note that ASSTYPE () does not have a replace = true. If you want to modify it on the original data, you need to write the following form. \\n\\n  app track [['uID', 'Index']] = App Train [['UID', 'Index']]. Astype (int) \\n\\n Note that only when the string of this column is composed of pure numbers, it can be written like this. If there is a mixed letter, an error will be reported: ValueError: Invalid Literal for int () with 10 \\n\\n Use the int () function to rotate string similar to \\n\\n ISDIGIT () is used to determine whether a string consists of pure numbers. If it returns true, otherwise false\",\n",
       " \"Training data set \\n  Embarked features have only 2 missing values, which can be easily filled. \\n  Age's features are more troublesome because it has 177 missing values. \\n  Cabin features need further investigation, but it may seem to be deleted from the data set, because the proportion of missing values is as high as 77. \\n  Test data set \\n  Fare features only 1 missing value, which can be easily filled. \\n  Age has 86 missing values. \\n  Cabin features need further investigation, but it may seem to be deleted from the data set, because the proportion of missing values is as high as 78.2. \\n\\n isnull usage \\n\\n  DF.ISNULL () element is empty or NA shows true, otherwise it is false \\n  df.ISNULL (). Any () determine which columns contain missing missing values. If there is a missing value in this column, it returns true, and false. \\n\\n Compared with count (), isnull (). Count () and isnull (). Sum (). \\n\\n  df.count () Each column in Africa is missing the number of missing values \\n  df.ismnull (). Count () The number of total elements of each column \\n  df.ismnull (). Sum () The number of data missing data per column\",\n",
       " 'Titanic survivor forecast\\n\\n Basic process of data science analysis\\n\\nDATA Science Solutions Book This book is a typical analysis process for solving data science competitions similar to the Kaggle website:\\n\\n Definition\\n Get training data and test data\\n Processing, preparation and cleaning data\\n Analyze, identify the data of data, and visualize the data\\n Modeling, prediction, and solving problems\\n Do visualization of results, generate reports, and show the problem of solution and the final solution of the problem\\n Submission result\\n\\nIn reality, it will not follow the above process strictly:\\n\\n It may be combined with multiple steps. For example, began to be visualized during the analysis process\\n Execute a certain step in advance. For example, data can be analyzed before and after data processing\\n Execute a certain step multiple times. For example, visualization of data multiple times\\n A completely discard one step. For example, there may not be steps to submit results in non -competitions\\n\\n Process goal\\n\\nThere are 7 goals:\\n\\n Category: It may be classified for our samples. We may also want to understand the connection between different categories and our goals.\\n Mutuality: We can solve a problem based on the characteristics available in the training set. Which characteristics in the dataset will play a vital role in solving the problem? Statically speaking, is there a certain connection between a certain feature and problem solution? If the value of this feature changes, will the corresponding problem solve also change? Is the case in turn? This can be tested by the numerical type and discrete characteristics in the dataset. We may also want to get the relationship between features, not the relationship between characteristics and problems. Finding the correlation between specific attributes may play a certain role in creating, completing, and correction characteristics\\n Conversion: For the modeling phase, we need to pre -process the data. According to the algorithm of the model, the feature value may be converted into a digital value. For example, the text feature value is converted into digital values.\\n Filling: In the preparation of data, we may also need to estimate the loss of some features. When there is no missing value, the model algorithm may be the best.\\n Amendment: We can also analyze the errors in the data concentration and the possible abnormal features that may exist, so as to correct these samples containing error values. One method is to detect abnormal values \\u200b\\u200bin samples or features. If a feature is not suitable for analysis, or it will seriously affect the results, we can also completely abandon it.\\n Creation: We can create new features based on the existing characteristics or characteristic sequences, so that the new features meet the correlation, conversion, and integrity goals\\n Drawing: How to make appropriate visual diagrams on the data according to the original data set and to solve the problem\\n\\n\\n\\n Definition\\n\\n\\n\"In the training data, the specific data of the passengers on the Titanic and the information they survived in that disaster. Can participants train a model through existing training data? This model needs to be based on the input input. Test the passenger information in the data to predict whether the passenger can survive in disaster \"\\n\\nWe may also want to obtain more information about this issue through the description of the problem. In the description of this issue, the more meaningful description is as follows:\\n Titanic sank after colliding with the iceberg on April 15, 1912. Among the 2224 passengers and crew members, 1502 people died unfortunately. This information shows that the return rate in the incident is 32.\\n The reason why there is such a large mortality in this disaster is that there is no sufficient lifeshw on the boat to provide passengers and crew\\n Although there is still luck in this disaster, there will still be some groups of survival rates higher than others, such as women, children, and people with superior warehouses.\\n\\n Code\\n\\n Reference article\\n [How I scored in the top 9 of kaggle ’s titanic machine learning challenge\\n] (https: Medium.com I like big data and I cannot like I scored in the top 9 of kaggles titanic machine learning challenge 243B5F45C8E9)))))))))))))\\n [How to score 0.8134 in Titanic Kaggle Challenge (https: www.ahmedbesbes.com blog kaggle titanic completion))\\n [Predicting The Survival of Titanic Passengers]\\n\\n Import the relevant library\\n\\nImport the required library, and at the same time, some initialization settings are performed.',\n",
       " 'Our data is now the format we need, using the previous Train IDX and Test IDX index separation training sets and test sets. \\n\\n The training set will also be divided into X, which represents predictable variables. Y represents our target variables, that is, the SurVived feature. \\n\\n  Intersection Question: The format of int64 before the combination of the SurVived features. \\n  Intersection Question: Warning needs to be resolved',\n",
       " \"Treatment feature Cabin creates a new feature DECK \\n The cabin number 'C123', which looks like the letter represents the deck number. Therefore, we want to extract these and create a new feature, which contains a person’s deck area\",\n",
       " 'Knowledge point: value counts () method \\n\\n Returns a sequence Series that contains the number of each value. In other words, for any column in the data box, the value counts () method returns the count of each item. \\n\\n  Cross each value that appears in this column (invalid value will be excluded) default order sorting \\n  Value Counts (Ascending = TRUE) \\n  Seeking relative frequency of each value \\n  Value counts (normalize = true)',\n",
       " 'Modeling',\n",
       " 'This DataFrame will help us estimate the lack of age based on different standards. Check the medium age column and see how this value is combined according to Sex, PCLASS and Title. \\n E.g: \\n\\n  If passengers are women, they are from PClass 1 and Royalty, with a median age of 40.5 years. \\n  If the passenger is a male, from PCLASS 3, with MR titles, the median age is 26 years old. \\n\\n Use the median age of different combinations to fill in the lack of age.',\n",
       " 'The Survived column is a target variable, which is a variable we want to predict. If Survived is 1, passengers are spared, and 0, indicating that they have not survived. \\n\\n Whether Survived survives \\n \\n 0 death \\n 1 survive \\n\\n The variables of other passengers are characteristics. \\n\\n Data feature meaning \\n \\n Passengerid passenger ID \\n PClass passenger level (1,2,3) \\n Name passenger name \\n Sex Passenger Gender (FEMALE, Male) \\n Age passenger age \\n Number of sibs and spouses of sibsp with passengers \\n The number of parents and children with passengers with passengers \\n TICKET ticket number \\n Fare fare \\n Cabin cabin number \\n Embarked passenger boarding port (C = Cherbourg, Q = Queenstown, S = SOUTHAMPTON) \\n\\n\\n  General information of the data set \\n\\n  df.info () shows general data information, including the name of each column, the number of non -vacant values, the data type of each column, memory occupation',\n",
       " 'Inference time',\n",
       " 'Training',\n",
       " 'Save pipeline',\n",
       " 'Evaluate',\n",
       " 'Model',\n",
       " 'Load data',\n",
       " 'Imports',\n",
       " '',\n",
       " 'It has been successfully added to the test data.',\n",
       " 'I feel a little deeper when I touch it all!',\n",
       " \"It is Osio of the secretariat. \\n To get used to Python today, Mr. Yamamoto's sample code of the last competition \\n https: www.kaggle.com Nejumi 2 Pandas \\n I tried to touch Pandas with the main data prepared in the competition.\",\n",
       " 'Favorites Rate has been added to the rightmost row!',\n",
       " 'OSHIO.CSV is saved in the Output folder at the top right of the screen!',\n",
       " 'From this graph it can be seen that the most expensive real estate is in the city of Bellevue, as well as the distribution of prices in Sammamish has a tail that goes into law. \\n The most low real estate and the number of real estate belongs to G.Auburn.',\n",
       " 'Such shortcomings should be found precisely during the EDA analysis, since, first of all, this gives us a deeper understanding of the data structure, and secondly, when building models, such as regression models, based on these data from such outlays, you should get rid of the model to be more accurate .',\n",
       " 'Our data includes real estate, the cost of which is 0.',\n",
       " 'On the graph on the X axis, too much data is reflected, which leads to difficulties in analyzing the schedule. One of the solution is the creation of temporary bacets that will only be announced on the sale of real estate objects.',\n",
       " 'Displot shows us that the price of the immovable survey is non -normally distributed and has an elongated tail in the right, which says that the Dataset includes non -voyage with high cost.',\n",
       " 'We understand that the price of the house cannot be 0, such a picture tells us about data insufficiency on these objects. Perhaps, when collecting this dataset, it was not possible to find prices for all objects that were put forward for sale. Perhaps the prices for such houses were taken to request separately and straightforward from the seller. \\n\\n In the date of the set there are houses without bedrooms and bathrooms, which can also explain the inaccessibility of data.',\n",
       " \"This case analyzes House Price Data Set when using the EDA analysis. \\n\\n The goal is to demonstrate the ability to work with raw data their preparation, cleaning for subsequent analysis, research and visualization of data. \\n\\n Let's Start\",\n",
       " 'So in Dataset there are 4600 lines and 18 columns, which can categorically be divided into: \\n\\n  Internal characteristics of the house (Bathroms, SQFT Living (Housing Square), SQFT LOT (total living area) and others ...) \\n  House locations (Street, City, Stateizip, Country) \\n  Date and price \\n\\n Types of our variables: \\n\\n Discrete variables Bedrooms, SQFT Living, SQFT LOT, BEDROOMS, SQFT ABOVE, SQFT BASMEEMENT, YR BUILT, Date \\n\\n Environmental variables Price, Floors \\n\\n Nominative variables Street, City, Statezip, Country, Waterfront \\n\\n Rang variables View, Condition',\n",
       " 'From the tables presented, it can be seen that the number of real estate with the absence of bedrooms and bathrooms is slightly and is complishable by 2 objects.',\n",
       " 'The matrix shows that a number of variables have a positive correlation:\\n1. Price and living space in real estate (coeff. 0.43) The more living space, the higher the price of the house\\n2. The number of bedrooms with a living space and with a number of bathrooms (Kheff. 0.59 and 0.55), which also makes sense since the American real estate market is customary to build bedrooms along with bathrooms.\\n3. The number of bathrooms and the living area are correlated even stronger than variables in clause 2 (Kheff. 0.76), which is also logical since with an increase in the residential area the number of bedrooms can remain pretense as their area will increase, but the bathrooms are mainly standard and are standard The amount increases with the growth of living space.\\n4. VD and the area of \\u200b\\u200bthe basement (Kheff. 0.45) The larger the house, the more the place to the basement is allocated\\n5. The number of floors with the number of bathrooms (Kheff. 0.49) since it is customary to build a room on each floor on each floor\\n6. The number of floors and the year of the seniority (Keff. 0.47) will make out that in recent years, unexplosive began to cost more floors\\n7. The year of construction correlates with the number of bathrooms and the number of storeys of the house (Keff. 0.46 and 0.47)\\n\\nAlso in the matrix there are negative correlation coefficients:\\n\\n1. The number of floors and the rate of real estate negatively affect the quality conditions of real estate (0.26 and 0.4 of the coeff)\\n2. In multi -storey buildings, it is customary to cost small basements (0.26 coeff)\\n3. The year of construction and the year of renovation have a negative coefficient, since the conduct of renovation requires a passage of a long period of the existence of real estate (KEEFF. 0.32)\\n\\nThese are the main conclusions that can be drawn from the correlation matrix. Such a picture uses a little closer to understand the interactions of variables, however, such coefficients should also be checked for significance.',\n",
       " \"From the graph it can be seen that the most affordable real estate is located in the city of Seattle, in the city of Kent. But let's try to remove the most broken values to see the distribution in other cities.\",\n",
       " 'Analysis of the distribution of the number of real estate by years shows that most often new real estate objects are put up for sale from 2000 and above the years of construction.',\n",
       " 'Dataset we have no missed values, but the Date variable has an incorrect Object format and it must be changed.',\n",
       " 'This Barplot illustrates another Outlair Oblane, which has 9 bedrooms, but has less than at the house with 4 bedrooms. Such behavior can be explained by the fact that the house can be far from the city center or a parking lot, where the demand for such real estate is very low, Therefore, real estate is unlimited with a low price.\\n\\nAnother conclusion that can be drawn from the schedule that real estate with 7 8 bedrooms has the largest price scatter. The buyer in the market can find real estate with 8 bedrooms at the price of real estate with 1 oh and 2 bums.',\n",
       " \"Small data processing is completed, so let's move on to a more dative data research.\",\n",
       " 'According to the average, the profile of real estate for selling real estate has a price of 551963, 3 bedrooms, 2 bathrooms, 198.7 square meters of living space, as well as 28. sq.m of the basement and has a quality assessment of 3.45. Sold houses already have a secondary real estate market since the average year of construction is 1970.',\n",
       " \"In our Dataset, all real estate objects are in the US country in one state of Washington. The number of cities, in the short houses 44 at home and in the TOP 10 cities are included in the ATS of real estate, “Seattle ',' Renton ',' BELLEVUE ',' REDMOND ',' ISSAQUAH ',' KIRKland ',' KENT ',' AUBURN ',' 'Sammamish', 'Federal Way'.\",\n",
       " \"And in the conclusion of our preparation of data, let's transfer the measurements of the SQFT Living and SQFT BASement from sq. Foot in sq. Mdl a more clear understanding of the volume of real estate site.\",\n",
       " 'Let me see how many of them are home, where the data detachment can be replaced by 0.',\n",
       " 'The next important step in the data analysis is the construction of Correlaration Matrix. The correlation coefficient indicates a strong interconnection of variables, that is, such a matrix helps us to find out how much variables are correlated with each other. However, in order to talk about a significant correlation of variables in the future, a statistical analysis will already be required.',\n",
       " 'We will end the process of data analysis and exploration \\n\\n Later in the project, we will go to the further part of the selection and the preparation of data for the regression model and the selection of the model itself and its parameters',\n",
       " 'We continued to analyze the generated features in the context of creating a regression model',\n",
       " 'The Sta LTA (Short Time Average Over Long Time Average) algorithm is designed to ignore energy signals from the vibration of the environment. He chooses only those parts of the signals whose amplitude is pseudo stationary. The algorithm is aimed at avoiding energy explosions. Below is its implementation.',\n",
       " \"In the chart below we took 9 features randomly and the next we presented their values compared to the 'Time to Failure' output variable ' \\n\\n As you can see with some variables, we can see the reliability, namely there is a sharp increase or decrease in the variable value along with the increase in the Time to Failure output variable.\",\n",
       " 'So we decided to examine the correlations between all features and the value of the output variable \\n\\n In this step, we decided to determine individual correlations between the features so as to see which they have the best results. We took into account correlations with an absolute value greater or equal to 0.3. As you can see as a result of the following piece of code of such correlations we have: X. The best of them have values above 0.6, then we should focus our most attention and they should have the greatest impact on our model.',\n",
       " 'At this point, we generated features (features) for each measurement of acoustic data found in the test set. Defined features and results assigned to structure x test were calculated for each data segment.',\n",
       " 'Below we have gathered all necessary and used libraries and frameworks, which made it easier for us to work in calculations, draw charts, classify, regression, exploration and analysis of data, prediction. \\n\\n We used the Seaborn and Matplotlib libraries for graphic visualization of charts. \\n The Catboost Library has been used in our project in tasks related to regression, machine learning. \\n The widest range of applications turned out to be scale, numpy and seaborn biloteics for machine learning, visualization and mathematical operations, which provided great tools for exploration and data analysis.',\n",
       " 'Initially, we drew charts from the middle of the training collection, and in this particular case below we took every 50 given from the training collection. Acoustic data (Acoustic Data) has been marked in red, and time (Time to Failure). Despite this, we could still notice some dependencies. In the chart below we presented the dependence of acoustic data (Acoustic Data) since the earthquake (Time to Failure). These data are in pure form not still unmodified at this stage. As you can see at every peak (peak) of acoustic data, we see a momentary uprising (peak) time from almost zero upside values, which symbolizes the earthquake, and then a linear change of time running again to zero.\\n\\nAs you can see in the picture below, you can find 7 situations that correspond to 7 earthquakes in this particular chosen part of the training collection. Most importantly in this chart is the fact that the peak of signal values \\u200b\\u200boccurs before almost every occurrence of the quake. For this fact, it will definitely be worth paying attention to charts showing maximum values \\u200b\\u200band standard deviation.\\n\\nIt is worth mentioning that in the entire training collection there is only 16, compared to all measurements, it is a value below 1',\n",
       " 'The function responsible for drawing a chart showing the average acoustic data with time to the earthquake for features provided in the function of the function. With the right choice of features, we are able to determine after drawing charts whether they are useful or not. In the next steps we will be able to see what the call for individual features returns, thanks to which it will be easier to determine their validity.',\n",
       " 'As we mentioned above, there are 7 pitches of all 16 on the chart, which appear in the full data set. We assumed that working on data, in which there is almost half of the quakes, can bring similar results, and will certainly be faster compared to work at the PEŁYNM collection. Analyzing only part of the collection, we can certainly come across the problems related to the fact that after each stroke of acoustic data there is an earthquake. We considered adding a feature (feature), which would contain the time value from the moment the peak (peak) of the acoustic value and the actual earthquake. Times for everyone would be very close and analyzing subsequent charts would be an average of 0.31 ms.',\n",
       " 'As you can see in the above chart, our assumptions turned out to be correct and features in which there is a sharp increase or decrease in values have high correlations with the output variable',\n",
       " 'After determining the indexes we are interested in, we drew charts of these values in a sample of 150,000 to look more closely at the peaks found. The signal peak already mentioned many times just before the quake is repeated quite often. This is certainly important and useful in influence, but if we take into account that such situations in the entire set divided into pieces with samples of 1,50,000 are only 16, it may turn out that it will be difficult to use it in the right way on the rest of the data.',\n",
       " 'In the case of standard deviation for different parts of the data, the situation looks like in the chart below. Values for training data differ significantly from test data. These are differences of around 1500 2500.',\n",
       " 'As you can see in the chart below, the values of the features dedicated to both collections differ from each other. This is the most visible in the case of average and sum. On the charts, the discrepancy may be a little smaller but still noticeable.',\n",
       " 'At this point, we have determined the number of earthquakes in the data on which we work and the number of peaks (peak) in this data. The numbers corresponded to each other, we set indexes of these peaks.',\n",
       " \"In our initial simplest model, the features as we determined mainly based on the basic measures of the central tendency. In the extended version we left earlier features (feature) and added new as you can see below. The newly added features include such things as: discreet Fourier transform, average absolute deviation, absolute value of the average difference, average variability factor, minimum, maximum, medium, minimum, minimum, maximum, the first, last fifty thousand and ten thousand. Differences between minimal and maximum values, average variability factor for different parts of the set. In addition, we used quantile, standard deviation, actions on the real and imaginary part of Fourier's quick transform. Functions for transforming signals Hilbert transformation, Hann function, Sta LTA algorithm. We also used the exponentially weighted movable average. We carried out regressions on selected deals. We used a function conducting the so -called Rolling Window Calculation for selected window size (Moving Window).\\n\\nWhen creating features, we used the discussion on Kaggle, and the kernels below and our own knowledge:\\n https: www.kaggle.com Artgor Even More Features\\n https: www.kaggle.com artor earthquakes fe more features and samples? scriptversionid = 9803210\\n https: www.kaggle.com abhishek quite a few features 1 51\\n https: www.kaggle.com Artgor Earthquakes Fe More Features and Samples\\n https: www.kaggle.com Vettejeep Masters Final Project Model LB 1 392\",\n",
       " 'The function responsible for drawing a chart showing the histograms of selected features for the training and test collection, which is useful in determining whether the results of the characteristics from the data sets are similar or differ from each other. In the next steps, calling the function will show us that the values of the harvest features differ very much from each other, and histograms hardly overlap. For this reason, we will try to reduce this discrepancy and scale the measurement values if necessary.',\n",
       " 'Later in the analysis, we will stop for the designation',\n",
       " 'Below are charts showing training data in the field of time, which shows moments of earth trumpet, of which, unfortunately, it is hard to deduce anything.',\n",
       " 'Then we generated random grain and random plaques of values from the range corresponding to the half of the training set. We drew a chart with a step every 150,000, because this was the number of samples in each of the measurements. If you look at the 9 charts below, we can see that the results of acoustic data in most cases in the field (200 300). However, as you can see, there were also measurements with results much larger, e.g. a result at the top around 1500 and about 4000.',\n",
       " 'We started the project with basic activities such as defining paths for loading files, basic imports of such libraries as NumPy and Pandas. Initially, we loaded only 1 2 data from the training set due to large and performance problems. The size of the training set exceeds 10GB, which prevented normal work on the file. Half of the collection is sufficient to present the basic measures of the central tendencies, from which we were later able to draw the appropriate conclusions. In addition, we loaded data from the test set and checked how many samples each segment of test data contained. Each of the files contained 15,000 samples. It is also worth mentioning that training and test data that have been made available for the needs of the competition differ very much from each other, because measurements for test data are not a direct continuation of the measurement for training data.\\n\\nThe results of the exploration were presented in the next stages of work on charts and tables',\n",
       " \"Let's question how balanced our data frame is distributed.\",\n",
       " \"Let's view how many unique values of our data frame Rating.\",\n",
       " \"Let's calculate and visualize the average rating rates in each category.\",\n",
       " \"Let's calculate the upload rates of each category. Let's show the top 15 categories of the most installed.\",\n",
       " \"To better understand, let's draw a distipot on rating.\",\n",
       " \"Let's observe how many missing value in which attribute is in the data framework.\",\n",
       " \"Let's view how many attributes and how many observations of our data frame.\",\n",
       " \"Let's view the first 5 observations of our data frame.\",\n",
       " \"Let's examine the distribution of the rating variable by drawing the violin graph. \\n\\n What does the distribution mean to us, can we say that it is a normal distribution?\",\n",
       " \"Let's print average values instead of NULL values in our target variable. \\n\\n To do this, let's view the basic statistical values we have viewed earlier. Let's print the rating value of the majority to null areas. \\n\\n Then let's check if the rating value is left.\",\n",
       " \"Let's install our necessary libraries.\",\n",
       " \"Let's view the basic statistical values for the numerical variable in our data framework. \\n\\n Let's make an idea of how variance of the variable has to inference from standard deviation and average value.\",\n",
       " 'Thank you for listening so far ..',\n",
       " \"Let's view the last 5 observations of our data frame.\",\n",
       " \"Let's view the unique values of our data frame's rating and Install attributes.\",\n",
       " \"Let's view the type of variables in our data frame and the usage of memory.\",\n",
       " \"Let's install our data frame and assign it to the DF variable.\",\n",
       " 'Discovery Data Analysis Google Play Store Apps',\n",
       " \"Let's find and visualize the top 10 applications that have been uploaded.\",\n",
       " \"Let's perform a grouping process according to the category attribute and visualize it.\",\n",
       " 'Comment: Frauds have a higher average transaction amount',\n",
       " '1. Logistics Regression',\n",
       " 'Comment: Data set is significantly reduced, about 50 memory compared to the beginning',\n",
       " '4. Remove Collinear Features (Delete highly similar characteristics)',\n",
       " '2. Data source used \\n Vesta Corporation has provided data sets for this contest. Vesta Corporation is a pioneer company in guaranteed e -commerce payment solutions. Founded in 1995, vesta pioneering in the process of card payment transactions (CNP) is fully guaranteed for the telecommunications industry. Since then, Vesta has strongly expanded the ability to science and data science and machine globally and consolidated the leading position in the field of e -commerce payment. Today, vesta guarantees more than $ 18 billion in annual transactions.',\n",
       " '1. Reduce memory',\n",
       " 'Based on the graph and comparison table, the selected model is LightGBM',\n",
       " 'See the amount of data, with each file we will have the corresponding number of rows and columns',\n",
       " 'Comment: \\n  Most of the transactions are cheating \\n  Only about 3.5 transaction data is fraud \\n  If using this data frame as a basis for prediction and analysis models, it is likely that many errors and algorithms may be redundant because it will assume that most transactions are not not. cheat. But the purpose of the model is to detect samples with signs of fraud. Therefore, attention should be paid to overfitting issues during analysis.',\n",
       " 'II. Data analysis',\n",
       " '6. PCA for V Columns',\n",
       " \"Comment: \\n  Productcd of W is the largest number \\n  S's Productcd has the least quantity\",\n",
       " '4. Random Forest',\n",
       " 'Distribution of transaction charges (transactionamt)',\n",
       " 'III. Data processing',\n",
       " \"Comment: \\n  C's ProductCD has the most fraud with about 12 \\n  W's Productcd has the least fraud with about 2\",\n",
       " 'Add necessary libraries',\n",
       " 'BECAUSE. summary',\n",
       " 'DeviceInfo',\n",
       " 'One Hot encoding is not used to eliminate multi -dimensional complexity. Although Label encoding is a good solution for ordering data, it is not a suitable solution for nominal classification data. It can be said that the frequency encryption is the best solution for this data.',\n",
       " 'DeviceType',\n",
       " '5. Frequency encoding (Frequency encoding)',\n",
       " '5. LightGBM',\n",
       " \"Because almost all the values of 'transactidid' are unique, it will be removed.\",\n",
       " 'Test data and train data with 393 columns do not include ISFRAUD data. Based on TransactionDT, the test data is assumed to have been created after the train data.',\n",
       " \"1. Introducing the problem\\nYou are standing in a payment counter and the cashier tells you that your card is rejected. Meanwhile, you are sure that your card is completely abundant to pay. You try again but the results are still rejected. When standing to the edge to make room for others, you receive a message from the bank about whether or not to pay.\\n\\nAlthough it is probably bulky and can be annoying for you at the present time, this fraud prevention system has saved consumers millions of dollars each year. Researchers from IEEE Computational Intelligence Society (IEEE CIS) want to improve this figure, and improve customer experience. With the ability to detect fraud with higher accuracy, you can use your chip without trouble.\\n\\nIn this competition, you will evaluate the benchmarking of machine models on a large -scale data set. Data comes from e -commerce transactions in the real world of vesta and contains a range of features from the device type to the product's features. You also have the opportunity to create new features to improve your results.\",\n",
       " 'With so many characteristics, the performance of the algorithm will decline seriously. PCA is a very popular way to speed up machine learning algorithms by eliminating correlation variables that do not contribute in any decision -making process. The time for training algorithms has decreased significantly with less features. In addition, Overfitting mainly occurs when there are too many variables in the data set. Therefore, PCA helps overcome the problem of excessive equipment by reducing the number of features. Therefore, if the input size is too high, the use of PCA to speed up the algorithm is a reasonable choice.\\n\\nWe observe that V has a large number of columns (about 340) but there is no contribution in the decision -making process. Therefore, we can ignore all V columns by applying PCA to all V columns to reduce the number of memory columns.',\n",
       " '24.4 Transactionid in Train Transaction (144233 590540) has a linked train. \\n  28.0 Transactidid in Test Transaction (141907 506691) has a linked Train Identity.',\n",
       " '3. Data file used \\n The data is divided into two Identity and Transaction files, combined by TransactionID. Not all transactions have corresponding identification information. \\n\\n For each transactionid in a test set, you must predict the probability of an online transaction that is fraud for Isfraud. \\n\\n  Train {transaction, Identity} .csv training set \\n  Test {transaction, Identity} .csv test set (you must predict ISFRAUD value for these observations) \\n  Sample submission.csv a submission file sample in the format',\n",
       " 'FRAUD DISTRIBUTION (Fraud)',\n",
       " 'Productcd distribution chart',\n",
       " '3. Predict submission data',\n",
       " '3. Handling the lack of values',\n",
       " '3. DecisionTree',\n",
       " 'V. Machine model',\n",
       " '2. Run with all data',\n",
       " 'Train and test distribution by division (transactiontt)',\n",
       " '2. Data evaluation',\n",
       " '1. Read data and display data \\n For each train transaction, test transaction, train identity, test identity, we will read and display some rows to review data overview. We will use some functions available in libraries to do this.',\n",
       " 'P emaildomain',\n",
       " 'I. Overview of the problem',\n",
       " '2. Data preparation',\n",
       " 'Convert it into a total number of days, weeks and hours',\n",
       " '1. Comparison between models',\n",
       " \"Remove Collinear Features (Delete highly similar characteristics) in the data frame with a correlation coefficient greater than the threshold. The elimination of these characteristics can help the model generalized and improve the model's interpretation.\",\n",
       " 'All data sets have missing values, which are understood to be popular in the real world.',\n",
       " 'The data set has too much value.',\n",
       " 'Chart percentage of Na value for each object',\n",
       " \"Working with large data while training ML model requires large RAM memory. To overcome this limitation, we use a function to reduce the memory capacity of data. The general approach is to convert the Dtype type of each feature ('int16', 'int3', 'int6', 'float16', 'float32', 'float64') into the lowest Dtype type possible.\",\n",
       " 'TransactionID',\n",
       " '3. Analysis of data characteristics',\n",
       " '4. Method of implementing the problem \\n  [Image.png] (Attachment: 0E43BD8A EDF4 432A A9FF 2212B98F0AFF.PNG)',\n",
       " '2. K nearest neighbor KNN',\n",
       " '6. XGBoost',\n",
       " 'Comment: \\n  Transactiontt feature is a timer feature from a certain reference date (not actual time mark). An early detection of data is the train training and testing seems to be divided over time. There is a small gap in the middle, the train is from a period of time and the test is from a later period. This will affect diagonal confirmation techniques that should be used.',\n",
       " '3) Random Forest',\n",
       " '6) encoding \\n We will encode the category variable as a numerical variable. Onehot encoding is applied to the category variable.',\n",
       " '1) CSV file load',\n",
       " 'ID column is a row identifier, so we will delete it.',\n",
       " 'Data split, Train (Train, Valid) \\n  Train Test Split parameter \\n  Test Size (Float): Specifies the ratio of the size of Valid (Test) \\n  Random State (int): When splitting the data, the random value used internally (not specifying that value is different every time.) \\n  Shuffle (bool): If you are mixing data, it will be mixed \\n  Stratify (ARRAY): Stratify is a value that must be set to maintain even after splitting the class ratio before splitting. You can put the class label. \\n  ex) If the class ratio of the original train data was (7: 3), the class ratio of the split Train, Valid (Test) data is also (7: 3). Naturally, it can only be used in classification data.',\n",
       " '6. Making results',\n",
       " '4. OOF (out of Fold) ensemble \\n It is an OOF ensemble that uses K FOLD to ensemble model verification and the results of each fold.',\n",
       " '2) Data check \\n Check the data with functions such as .s (), .describe (), .info ().',\n",
       " \"5) Data split \\n  1. Train, valid, test set \\n  Train Data: Data used to learn model (data to learn, past data) \\n  Valid Data: Data that verifies the performance of the learned model (data that the model doesn't know, data used for model verification, past data) \\n  Test Data: Data to be predicted as a learned model (predicted data, future data)\",\n",
       " '5) LightGBM',\n",
       " '1. Data pretreatment',\n",
       " \"2. Use the scikit Learn classification model \\n Let's use the default classification model of Scikit Learn. \\n Each model's evaluation metric uses the F1 Score, the competition evaluation metrics.\",\n",
       " '6) Scaling \\n We will use the Standard Scaler in the Scikit Learn library to standardize the numerical variables.',\n",
       " '4) Log conversion \\n Since the distribution of the CAPITAL GAIN variable and Capital Loss variable is biased to one side, we will adjust the shape of the distribution through log conversion.',\n",
       " \"3) Missing dental processing \\n In the previous lecture, I found out that there was a part -of -the -line column in 'WorkClass', 'Occupation' and 'Native Country' column. \\n Unlike the general meeting value, the values represented by '?' Will be processed as the most frequent value of the column. \\n\\n  In the case of categorical variables, the most simplest mops can be handled by the maximum, but you can filter other columns to process the side. ex) Education Num, etc.\",\n",
       " '1) Logistic regression model',\n",
       " '2) Support vector machine (RBF kernel)',\n",
       " '4) XGBoost',\n",
       " '2) 2 Stage Meta Model Learning \\n All variables in New Train and New Test are numerical variables, so we will only proceed with Standard Scaling. \\n New TRAIN and New Test data are created with the newly created data, learning 2 Stage Meta Model and creating results.',\n",
       " '5. Stacking ensemble \\n 2 Stacking ensemble, a stage ensemble. The Stacking ensemble is an ensemble method that collects the results of dozens of 1 Stage model and is a 2 Stage model. \\n\\n  1) Collect 1 STAGE results \\n The results of the 1 Stage model will be collected (Train, Test) to proceed with the stacking ensemble.',\n",
       " '4 1. OOF ensemble training (Logistic Regression, LightGBM, 20 minutes)',\n",
       " '3. K Fold Cross Validation \\n First, make the pretreatment process summarized in 1. as a function.',\n",
       " 'Sample Submission',\n",
       " 'Aim (accuracy of shooting) players \\n  [] (https: media.giphy.com Media 26xiwwwaw1d6uezfpu giphy.gif)',\n",
       " 'Normalize Kills, Damage Dealt, Max Place, Match Duration',\n",
       " 'Create Random Forest Regressor and build it according to our training data.',\n",
       " 'Groupid ID for identifying the group in the match. If the same group of players plays in different matches, they will have a different Groupid every time.\\n Matchid ID for identifying the match.\\n Assists The number of enemy players damaged by this player and killed by teammates.\\n Boosts The number of boosters used.\\n DamageDealt General Damage. Note: Damage is deducted by himself.\\n DBNOS The number of players who were knamnut.\\n Headshotkills number of killed players with shots.\\n Heals The number of used treats used.\\n Killplace a place that the player took in the match.\\n Killpoints glasses based on the player rating. (Think about it as an elo rating, where only murders are important).\\n Kills the number of murders.\\n Killstreaks The maximum number of enemy players killed in a short time.\\n Longestkill is the largest distance between the player and the player killed at the time of death. This can be misleading, since the detachment of the player from the game and departure can lead to great statistics.\\n Maxplace the worst place we have data in the match. This may not coincide with NumGroups, since sometimes data are passed by placement.\\n NumGroups The number of groups by which we have data in the match.\\n Revives how many times this player restored teammates.\\n Rededistance General Distance in Vehicles, measured in meters.\\n Roadkills The number of murders in the car.\\n Swimdistance General distance traveled in meters.\\n Teamkills how many times this player killed a teammate.\\n VehicleDestroys The number of destroyed cars.\\n Walkdistance common distance on foot in meters.\\n Weaponsacquuird amount of raised weapons.\\n Winpoints winning external rating of the player. (Think about it as an elo rating, where only a win is important).\\n WinPlaCeperc Purpose Purpose. This is a winning placement in percentiles, where 1 corresponds to 1 mu, and 0 corresponds to the last place in the match. It is calculated by maxplace, and not by NumGroups, so fragments can be missed in the coincidence.',\n",
       " 'We are looking for games in which WinPlaCeperc is Nan',\n",
       " 'We are looking for players who committed murders without the distance (without movements)',\n",
       " 'As we see Roadkills, Teamkills, Swimdistance and VehicleDestroys are weakly affected by our WinPlaceperC coefficient, so we will remove them. Also Rankpoints, Killpoint, Winpoints are also useless, but they are needed to search for a rating game so that the players come across equal opponents, so in the future we form one sign by drying them',\n",
       " 'We normalize data on connected players using logarithming.',\n",
       " 'Solution\\n [](https: www.elsetge.cat myimg f 80 808526 large size of pubg pubg pc.jpg)',\n",
       " 'Walk Distance',\n",
       " 'We delete this line, because they had only one player',\n",
       " 'Distance \\n  Now add a sign of Distance, which will be calculated by the formula \\n  [Distance] (https: Sun9 63.USERAPI.com C854228 V854228253 1CA694 FRF7LWVBK C.JPG)',\n",
       " 'We assign all the attires to the WinPlaCeperC column in the test date of the set and display it to the SubMission RF.CSV file with ID players',\n",
       " 'Exploratory Data Analysis\\n [](https: i.gifer.com Cwgf.gif)',\n",
       " 'Swim Distance',\n",
       " 'Data Description',\n",
       " 'Content \\n  [Data Description] (Data Description) \\n  [Exploratory Data Analysis] (Explorator Data Analysis) \\n  [Feature Engineering] \\n  [Solution] (Solution)',\n",
       " 'Players Joined \\n We create a sign showing how many players were in the game, this sign will help us normalize the data',\n",
       " 'Cleaning the Dataset from cheaters \\n  [] (https: media.giphy.com media pxot7kx1tuvos giphy.gif)',\n",
       " 'Divide our data into subgroups',\n",
       " 'You can safely remove those who passed more than 10,000 meters',\n",
       " 'Heatmap',\n",
       " 'Ride Distance',\n",
       " 'Determination of data from the Dataset',\n",
       " 'Consider the distances from which the murders were made',\n",
       " 'Now we subtract our data on variables X Train, X Val, Y Train, Y Val',\n",
       " 'We will delete those who have committed murders from a distance of more than 600 meters',\n",
       " 'Skill \\n Pabg game in which the personal skill of players is important, so we will create a sign of Skill \\n  [Image.png] (https: Sun9 8.USERAPI.com C200820 v200820016 314A1 07XLZJSJKMK.JPG)',\n",
       " 'We removed 2238 lines from our set set',\n",
       " 'Also, when the cheaters are detected, we created HS Ratio, which we will also need, we visualize the dependence',\n",
       " 'We got something like the Gaussian distribution with good players on the right and bad on the left',\n",
       " 'We will delete from our model those who have committed more than 45 murders',\n",
       " 'Heals \\n We will analyze another important aspect of the game, these are the so -called Hilki (Heals), they treat the character, i.e. restore health. \\n a sign of Healsratio, which is calculated by the formula \\n  [Hilki] (https: sun9 36.usrapi.com c854228 v854228253 1CA69B 6VOQNGWD3MW.jpg)',\n",
       " 'We will begin the very decision to begin to create previously identified criteria for the test dataset',\n",
       " '[PUBG](https: i.imgur.com 0hB3OfZ.jpg)',\n",
       " 'We also add the percentage of murders to the head, which will also help to identify cheaters',\n",
       " 'Feature Engineering\\n [](https: media.giphy.com media 39t19N2ICoIWkPaqay giphy.gif)',\n",
       " \"Now let's look at those who have committed an abnormal amount of murders in transport and delete them from our model \\n  [] (https: I.gifer.com 7jj0.gif)\",\n",
       " 'Bosterers \\n Bosterers are also very important, good players always try to be packed to the maximum, because they increase the regeneration of lives and the speed of movement, which is very important for victory. Therefore, we add a sign of Boostratio, which we calculate by the formula \\n  [Boosters] (https: sun9 36.USERAPI.com C854228 v854228253 1CA68D 64PRTDEQIIY.JPG)',\n",
       " 'Hs ratio',\n",
       " 'We will check our model according to the MAE metric we need (average absolute error)',\n",
       " 'We will delete those who drove more than 20,000 meters',\n",
       " 'You can calmly remove those who sailed more than 1000 meters',\n",
       " 'As we see, there are many people with 0 and 1 coefficients, it can be people who did 1 murder, but by chance, so we will change the values of the HS Ratio coefficient.',\n",
       " 'The result of cleaning',\n",
       " 'We create Y Train in which we put the target variable, and in the X train signs by which we will build Random Forest',\n",
       " 'We display our cheaters and delete them from our model',\n",
       " 'Killsratio \\n  Good players should do a lot of murders, so add Killsratio, and we will calculate it according to the formula \\n  [Frags] (https: sun9 127.usrapi.com c854228 v854228253 1CA6A2 CY3ZKH YOP8.jpg)',\n",
       " 'We form several new signs useful for us',\n",
       " 'We will delete this columns that we no longer need',\n",
       " 'We will build a spectrogram in the number of murders and the corresponding number of players',\n",
       " \"Let's see who has 10 or more murders from 100 hits and remove them\",\n",
       " \"We all added all the new signs, now let's look at the new correlation card\",\n",
       " \"Let's look at a thermal card with pieron correlation coefficients with their help, we understand what signs to pay attention to\",\n",
       " 'A simple submission',\n",
       " 'At the end of the forecasts, we rewrite the data to the original scale and calculate the RMSE metrics.',\n",
       " 'Here we create our data pairs x and y, where y {t} = x {t 1} (for look bak = 1). Another way to think about the value of y to a data x is that it is the next X in the temporal series.',\n",
       " 'Now try to improve the result of the RMSE indicator',\n",
       " 'Prints the forecast graph (in red)',\n",
       " 'Now we have created our 32 -recurrence LSTM network (you use up to 32 points in the series to try to predict the next number) and we train it.',\n",
       " 'In this laboratory we will demonstrate how an LSTM can be used for forecasting in temporal series. We will use a very simple LSTM neuronal network to treat a case of a time series of passengers who traveled per month on international flights in the United States.',\n",
       " 'LAB 6 Using LSTM to predict temporal series',\n",
       " 'Here we treat the data we will use. First we adjusted the data scale to be between 0 and 1. Then we divided the data between training (first 67 months) and tests (33 of the final months of the series)',\n",
       " 'We can display the confusion matrix:',\n",
       " 'We display 50 images where the algorithm was wrong:',\n",
       " 'Reading images',\n",
       " 'Deep CNN model',\n",
       " 'Hard et tensorflow',\n",
       " 'To install Keras and Tensorflow without GPU:',\n",
       " 'Convolutional networks: CNN',\n",
       " 'The trained model can be saved:',\n",
       " 'Learning can be a bit long without GPU ...',\n",
       " 'You can then use the model without starting training:',\n",
       " 'We can display the structure of the model:',\n",
       " 'and on the validation set:',\n",
       " 'A Convolutional layer',\n",
       " 'CONDA INSTELL C CONDA FORGE KERAS \\n CONDA INSTELL C CONDA FORGE TENSORFLOW',\n",
       " 'We will use use a convolutional layer for the extraction of the characteristics, and a dense layer for classification:',\n",
       " 'We define a function to display a score graphic:',\n",
       " 'Initialisations',\n",
       " 'We test a model with more convolutionary layers:',\n",
       " 'The Variable Train memorizes the history of scores on the learning set:',\n",
       " 'Pour installer la version GPU sous windows, cf https: medium.com @raza.shahzad setting up tensorflow gpu keras in conda on windows 10 75d4fd498198 \\nSous Linux : http: deeplearning.lipingyang.org 2017 08 01 install keras with tensorflow backend \\nSous MacOS (avec GPU Nvidia) : https: blog.wenhaolee.com run keras on mac os with gpu',\n",
       " '95.476 acc',\n",
       " 'Deficient value of numerical data',\n",
       " 'Bond Train and Test',\n",
       " 'EDA',\n",
       " 'MODELING',\n",
       " \"According to the description of 'Price Range', these are prices in a restaurant. \\n They can be put in increasing (it means this is not a categorical sign). And this means that they can be replaced with consistent numbers, for example, 1.2.3 \\n  Try to process this feature already yourself\",\n",
       " 'Eda \\n [Exploratory Data Analysis] (https: ru.wikipedia.org wiki Data analysis) Data analysis \\n At this stage, we are building graphs, looking for patterns, anomalies, emissions or communication between signs. \\n In general, the purpose of this stage is to understand what these data can give us and how signs can be interconnected. \\n Understanding the original signs will generate new, stronger and, thus, make our model better. \\n  [] (https: miro.medium.com max 2598 1 rxdmb7UK6MGQWQPGULAQ.png)',\n",
       " \"And someone said that the French love to eat =) Let's see how the distribution in the big city will change:\",\n",
       " 'Take a sign of Price Range.',\n",
       " \"Let's see the distribution of the target variable\",\n",
       " \"Let's create a function that will check the presence of a specific kitchen in the 'Cuisine Style' column for the current restaurant and return 1 if the kitchen is in the restaurant, and 0, if absent. We organize a cycle with a parameter in which we will sort out the names of all kitchens (Kitchen 300). For each kitchen, we will create a column with the corresponding name in the Datafreim and fill it with one and zeros, applying to the column 'Cuisine Style', the function we created in the previous step.\",\n",
       " \"There are many approaches for coding categorical signs: \\n  Label Encoding \\n  One Hot Encoding \\n  Target Encoding \\n  HASHING \\n\\n The choice of coding depends on the sign and the selected model. \\n We will not plunge very much now in this topic, let's see better an example with One Hot Encoding: \\n  [] (https: i.imgur.com mtimfxh.png)\",\n",
       " 'DATA',\n",
       " \"Now let's take the class of class Object Datetime64 and delete from Datapham\",\n",
       " \"Let's see the distribution of the target variable relative to the sign\",\n",
       " 'Model \\n Sam ml',\n",
       " \"What's Next? \\n Or what to do to improve the result: \\n  Process the remaining features in the format understandable for the machine \\n  See what else can be extracted from signs \\n  Generate new signs \\n  Load additional data, for example: by the population or well -being of cities \\n  Choose the composition of the signs \\n\\n In general, the process is creative and very exciting good luck in the competition\",\n",
       " 'What signs can be considered categorical?',\n",
       " 'Before sending our data to training, we divide the data into another test and a trial, for validation. \\n This will help us check how well our model works, before sending SubMissiona to Kaggle.',\n",
       " 'It turns out that Ranking has a normal distribution, just in large cities there are more restaurants, because of this we have a displacement. \\n\\n  Think about how to make a sign from this for your model. I will show you an example of how visualization helps to find relationships. And then act without prompts =)',\n",
       " 'For some algorithms ml, even for non -categorical signs, you can apply One Hot Encoding, and this can improve the quality of the model. Try different approaches to encoding the attribute no one knows in advance what can take off.',\n",
       " 'style kitchen',\n",
       " \"2. Processing of signs \\n To begin with, let's see what signs we can be categorical.\",\n",
       " 'Cleaning and Prepping Data \\n Typically, the data contain a bunch of garbage, which must be cleaned in order to bring them into an acceptable format. Data cleaning is the necessary stage of solving almost any real problem. \\n  [] (https: analyticsindiamag.com wp content uploads 2018 01 Data Cleaning.png)',\n",
       " 'Submission \\n If everything is happy, we cook submission for a kagl',\n",
       " 'Reviews',\n",
       " 'And one of my beloved [Correlation of Signs] (https: ru.wikipedia.org wiki correlation) \\n On this schedule, now you can notice how the signs are interconnected and with the target variable.',\n",
       " 'As you can see, most of the signs require cleaning and preliminary processing.',\n",
       " '1. Nan processing \\n The presence of passes may have different reasons, but the gaps must be either filled or completely excluded from the set. But with passes you need to be careful, even the lack of information can be an important sign \\n Therefore, before the processing of NAN, it is better to make information about the availability of a pass as a separate feature',\n",
       " 'Hand',\n",
       " \"[] (https: www.pata.org WP Content Uploads 2014 09 Tripadvisor Logo 300x119.png)\\n Predict Tripadvisor Rating\\n In this competition, we have to predict a restaurant rating in Tripadvisor\\n In the course of the task:\\n We pump work with Pandas\\n We will learn to work with Kaggle Notebooks\\n We will understand how to make the expense of various data\\n We will learn to work with missed data (NAN)\\n Let's get acquainted with various types of coding signs\\n Let's try a little [feature Engineering] (https: ru.wikipedia.org wiki design of signs) (generate new signs)\\n And we’ll touch up a little ML\\n And much more...\\n\\n\\n\\n And most importantly, you can do all this yourself\\n\\n This laptop is an example of a template for this competition (Baseline) and does not serve as a ready -made solution\\nYou can use it as a basis for building your decision.\\n\\n What is Baseline a solution, why it is necessary and why it is a important standard for the competition for the competition for Kaggle and other sites.\\n Baseline is created more as a template, where you can see how the appeal with the incoming data occurs and what needs to be obtained at the exit. In this case, ml filling can be quite simple, just for example. This helps to start faster to ml, and not spend valuable time on purely engineering tasks.\\nBaseline is also a good reference point by metric. If your solution is worse than Baseline, you are clearly doing something wrong and it is worth trying a different path)\\n\\nIn the context of our competition, Baseline goes with small examples of what can be done with the data, and with the instructions, what to do next to improve the result. In general, this is difficult to call a ready -made solution, since only 2 the simplest features are used (and the rest are excluded).\",\n",
       " 'We have many restaurants that do not reach up to 2500 places in their city, and what about cities?',\n",
       " 'The kitchens are less than 300 in one column for this will create a function that will return the amount of rare kitchens in each restaurant.',\n",
       " 'In general, thanks to visualization in this dataset, you can learn a lot of interesting facts, for example: \\n  Where is the Pitseria in Madrid or London? \\n  In which city is the kitchen of restaurants more diverse? \\n\\n Come up with your question and find the answer to it in the data)',\n",
       " \"Let's see the distribution of the sign\",\n",
       " 'import',\n",
       " \"Read more on the basis: \\n  City: City \\n  Cuisine style: kitchen \\n  Ranking: Restaurant's rank relative to other restaurants in this city \\n  Price Range: Prices in a restaurant in 3 categories \\n  Number of Reviews: Number of reviews \\n  Reviews: 2 recent reviews and dates of these reviews \\n  URL TA: restaurant page on 'www.tripadvisor.com' \\n  ID TA: restaurant ID in Tripadvisor \\n  Rating: Restaurant Rating\",\n",
       " 'Division of the present data set into training and test data set. \\n The model is trained with the help of the training data set and then evaluated with the test data record.',\n",
       " 'Missing values',\n",
       " 'Model with Random Forest',\n",
       " 'Optimize the model \\n It would be possible to optimize the previous model by adapting the branches (Leaves).',\n",
       " 'Comparison of the models together',\n",
       " 'There is a relationship between most of the selected features and the sales price. \\n The graphic for greater (base area) is very distorted due to the outlets, which would have to be excluded for a closer look at the relationship.',\n",
       " \"It shows that 18 characteristics contain missing values. \\n The characteristics 'Alley', 'Fireplacequ', 'Poolqc', 'Fence' and 'Miscfeatature' are most affected of the missing values. \\n\\n In order to obtain a realistic price forecast, these values should be examined in more detail on how they affect the sales price. \\n Since only the simple guide for the development of an ML app is shown here for the forecast of house prices, these values are excluded from training data set in order not to create any errors in the model. The features 'GarageCond' and 'Poolqc' that were previously identified as possible training features can thus be excluded from the list of the input variables. \\n\\n The current list of input variables is as follows:\",\n",
       " 'Model with a linear regression',\n",
       " 'Discrete characteristics',\n",
       " 'Equalization of numerical numerical values \\n Since the numerical variables are distorted, a normal logarithmic distribution is carried out for adjustment.',\n",
       " 'There is a relationship between the selected features and the sales price.',\n",
       " 'It shows that the 1460 houses (lines) and 81 characteristics (columns) contain.\\nThe characteristics of the houses include both numerical (numbers) and categorical (text) values.\\nThe documentation (data description) enclosed with the data set provides a precise explanation of all characteristics and their data.\\n\\nBased on the assumptions taken, the following characteristics are examined in more detail:\\n MSZONING: Identifies the general division of zone of the sale.\\n Lotarea: Property size in square foot\\n Neighborhood: Physical locations within the city limits of Ames\\n BLDGType: real estate type\\n OverallQual: evaluates the overall material and the execution of the house\\n Overallcond: evaluates the overall condition of the house\\n Yearbuilt: original building date\\n YearRemodadd: conversion date (corresponds to the building date if no conversions or additions)\\n Totalbsmtsf: Total square foot basement area\\n 1stflrsf: square meters on the first floor\\n Grlivarea: above -ground living space (square foot)\\n Fullbath: Financed bathrooms above the level\\n Tottrmsabvgrd: Total number of rooms higher (without a bathroom)\\n GarageCars: Size of the garage in car capacity\\n Garagearea: Garage size in square meters\\n Garagecond: Garage state\\n Poolarea: pool area in square meters\\n Poolqc: pool quality\\n YRSOLD: Sales year (Yyyy format)\\n Saleprice: Sales price in USD\\n\\n\\nThe documentation of the data shows that the data records are available in different data formats even if they are of the same type (e.g. Lotarea in square foot and poolarea in square meters). These different formats can have an impact on the ML model, such as lower accuracy and new data records should have exactly the same formatting to get a constant prognosis quality.\\n\\nFor this work that shows the procedure for the development of an ML app, only the data examination is simplified and only the correlation of these values \\u200b\\u200bwith the variable you are looking for is examined in more detail to determine whether it is suitable as part of the data set for the ML model.',\n",
       " 'Evaluation',\n",
       " 'Convert years',\n",
       " 'Model with linear regression',\n",
       " 'Model mit Decision Tree',\n",
       " 'Data analysis \\n\\n In the context of the data analysis, the data of the pre -selected list is examined as follows: \\n 1. Missing values \\n 1. Numerical features \\n  1st time characteristics (e.g. annual figures, date information) \\n  1. Discrete characteristics \\n  1. Continuous features \\n  1. Distribution of the numerical features \\n  1. Excess \\n 1. Categorical features characteristics \\n  1. Cardinality of categorical features \\n 1. Relationship between independent (input variables) and dependent characteristics (target variable)',\n",
       " 'Check the missing values \\n\\n Checking the percentage of nan values (missing values) in every feature.',\n",
       " 'Convert the categorical values of text into numerical values',\n",
       " 'Scaling the characteristics',\n",
       " 'For the evaluation of the models, the common evaluation metrics for regression are problems, these are: \\n\\n  Mean Absolute Error (MAE) is the mean of the absolute values of the errors: \\n  frac 1n sum {i = 1} n y i has {y} i \\n\\n  Mean Squared Error (MSE) is the mean of the squared errors: \\n  frac 1n sum {i = 1} n (y i has {y} i) 2 \\n\\n  Root Mean Squared Error (RMSE) is the square root of the average of the square errors: \\n  sqrt {frac 1n sum {i = 1} n (y i has {y} i) 2} \\n\\n  Comparison of these metrics: \\n  Mae is the easiest to understand because it is the average error. \\n  MSE is more popular than MAE because MSE punishes larger mistakes, which is more useful in the real world. \\n  RMSE is even more popular than MSE because RMSE can be interpreted in the Y units. \\n\\n  All of these functions are loss functions with the aim of minimizing their result value.',\n",
       " 'There is a relationship between the selected features and the sales price.',\n",
       " 'Remove less often categorical values \\n Replacing categorical variables that make up less than 1 of the observations with a collective variables.',\n",
       " 'introduction\\n\\nThe aim of this ML app is to develop a pricing model that estimates houses in the city of Ames and offers home owners or prospective buyers more price transparency.\\n\\n target\\n1. Training of a model for machine learning to predict future real estate sales prices in Ames, Iowa\\n1. Creating a simple guide for the development of an ML app for the forecast of house prices\\n\\n Target user\\n Private individuals (prospective buyers)\\n Owner\\n SMEs in the Immoblilien area without its own programmer or larger IT department\\n\\n Results summary\\nBased on the present data record, an ML model can be trained by using regression techniques such as linear regression, Random Forest or Gradient Boosting to predict real estate prices.\\n\\n\\n Content of this notebook\\n\\n Business understanding\\n Data Investigation: Explorative data analysis of this data\\n Data preparation: Cleaning and merging the data\\n Selection of the characteristics: Define the characteristics for the training set of the ML model\\n Selection and training of the ML model\\n Evaluation of the model performance\\n\\n Business understanding\\n\\n Target group: private individuals such as prospective buyers or homeowners and the real estate industry working in the real estate industry (small to medium -sized companies)\\n\\n Problem definition: Precise determination of the purchase or sales price of real estate\\n\\n Target variable: sales price\\n\\n Basic hypothesis: The value of a property depends on the properties of the property and its quality, the location of the property and the economic situation of the buyers.\\n\\n Enter varibals\\nIn order to facilitate the number of input variables and to facilitate further data analysis and to reduce the associated expenses, the following assumptions are taken:\\n\\n1. Type of property (e.g. house or apartment): houses are more expensive than apartments\\n1. Base area of \\u200b\\u200bthe property (base area): More space leads to higher prices\\n1. Age of the property: New buildings are more expensive than existing buildings\\n1. Condition of the property: New properties with materials of high quality quality can be bought more expensive than dilapidated with a lack of quality quality.\\n1st period of the last modernization of renovation measures: Measures that contribute to reservation lead to higher sales prices\\n1. Number of rooms: Larger number of rooms lead to higher sales prices\\n1. Luxury or functional facilities: such as garages, garden area or pools lead to higher sales prices\\n1. Location of the property: A property in the city center of a metropolitan area (e.g. Munich) is more expensive than in a small village in the country\\n1. Neighborhood or cultural environment: Depending on the surrounding neighborhood, the property can have a higher or lower sales price than a comparable property in a different neighborhood (e.g. a property in the middle of a social focal point or on the edge of a odor -intensive, processing industrial area is sold cheaper become than in a residential area with raised social and living standards)\\n1st year of sales: At the time of a economic crisis, real estate is cheaper to advertise than in times of low interest rates\\n\\n Data investigation\\n\\n Data record Ames Housing Dataset\\n\\nThe data record Ames Housing Dataset comes from the Kaggle.com Competition House Prices Advanced Region Techniques and can be found under this link https: www.kaggle.com C House Prices Advanced Region Techniques\\n\\nThe data record contains 79 explained characteristics that describe almost every aspect of residential buildings in Ames, Iowa (USA). Therefore, he owns an ML model very well based on this data for the above -mentioned problem description.\\n\\n\\n Importing the required python libaries',\n",
       " 'Except in the numerical characteristics',\n",
       " 'Modelling',\n",
       " 'Time features (e.g. years, date information)',\n",
       " 'Specify and train a Random Forest model',\n",
       " 'Categorical characteristics',\n",
       " \"Data Preparation\\nDuring the data preparation, the input variables are determined and brought to a suitable format for the model.\\nSince the task of the model is a forecast for a numerical value (a so -called regression's task), all input variables must also be numerical values.\\n\\nPreparing the data for modeling using regression procedures\\n\\n Linear assumption: In the linear regression, it is assumed that the relationship between your input and output is linear. It does not support anything else. This may be obvious, but it is good to remember when you have many attributes. You may have to transform data to make the relationship linear (e.g. log transformation for an exponential relationship).\\n Remove Noise: In the linear regression, it is assumed that their input and output variables are not noisy. In consideration of using data adjustment processes, with which you can better uncover and clarify the signal in your data. This is most important for the output variable and you want to remove outliers in the output variable (y) if possible.\\n Remove Collinearity: The linear regression adapts your data too much if you have severely correlated input variables. Keep considering calculating correlations for your input data in pairs and removing the most correlated.\\n Gaussian Distributions :. The linear regression provides more reliable predictions if your input and output variables have a Gaussche distribution. You can achieve some advantages by using transformations (e.g. Log or Boxcox) for your variables to make your distribution look like.\\n Rescale inputs: The linear regression often provides more reliable predictions if you scalate input variables using standardization or normalization.\\n\\nThe following measures are carried out in the following:\\n\\n1. Compensate for missing values\\n1. Convert years into comparable temporal variables (intervals)\\n1. Inclusion of the numerical values, since the different sizes of the feature values \\u200b\\u200bcould unintentionally influence the model\\n1. Removing rare values \\u200b\\u200binto categorical variables\\n1. Convert the categorical variables of text values \\u200b\\u200b(string) to pay (float)\\n1. Stand up the values \\u200b\\u200bof the variables in the same area\",\n",
       " 'Continuous characteristics',\n",
       " 'Specify and train a decision -making tree model',\n",
       " 'Load the data and create an overview of the storm of this',\n",
       " 'Numerical characteristics',\n",
       " \"Now let's break our X and Y variables that we will use to train the models.\",\n",
       " 'The Random Forest Register model, seems more consistent in this scenario, we will analyze the sample oft.',\n",
       " 'Same amount of variables, ok.',\n",
       " 'Selection of variables.',\n",
       " \"We will see that our Baseline is at 14,910, let's check how other models behave.\",\n",
       " 'Random Forest Regressor e Gradient Boosting Regressor',\n",
       " 'With all the variables filled in the Dataset, we will analyze the possible correlations between them.',\n",
       " 'The types of store types appear to be done according to the size of the store. \\n We will group the information from features and stores to continue the analysis of variables.',\n",
       " \"Even not using the time series approach, let's make a training break and test considering that our sales behave with a time function, so we will break the test series in Out of Sample and Out of Time\",\n",
       " 'We can identify that there is some fluctuation in sales on holidays, we will place the holidays provided to identify possible interactions.',\n",
       " 'Breaking the training sets and tests',\n",
       " 'Training and testing',\n",
       " 'The models',\n",
       " 'We note that the Markdowns have a lot of outliers per store, so we will treat missing values using the median values per store and for a holiday or not on the date.',\n",
       " 'Conclusions',\n",
       " 'Treating Missing Values',\n",
       " 'At first sight, considering sales over a certain period of time to predict future sales, it was considered the use of the approach by temporal series. For this we should analyze time sales and try to adjust an Arimax class model to design future sales. \\n Analyzing features, we note that there are future dates, or dates that we will estimate, data we can use some classification regression model. \\n If we were using an Arimax model, we would take the department variables, seeking to analyze only weekly sales. \\n We have done a small analysis of sales and realized that the differences between weeks have a stationary behavior, which would allow a time series approach, which was not the case.',\n",
       " 'Analysis of variables',\n",
       " \"We can check some logical correlations between holidays, but we can also notice some correlations between Markdowns. Let's remove some variables in order to simplify the model and mitigate multicolinearity problems as we have not yet decided which model we will use in our predictions.\",\n",
       " 'Teste Out of Time',\n",
       " 'A round of gridsearch was made to determine the best parameters of the models in question. With these parameters we will calibrate and do the predictions and data analysis.',\n",
       " 'Calibrating and calculating the effectiveness of the models',\n",
       " 'The Random Forest model proved more consistent with tests with these parameters. The Gradient Boosting Regresent had a better effectiveness in testing with other parameters, which demand much more of our computational power, so we chose not to use it.',\n",
       " \"We can see that type C stores have less variability over time, maybe these stores do not have so much influence of holidays, let's look further.\",\n",
       " 'As our data is granular at department level, we will group by store to try to better identify trends',\n",
       " \"Before we start model tests, let's treat missing values. \\n Note that Markdowns are the only missing values on this dataset, and considering the documentation and nature of these features we will make a quick analysis to verify the behavior of these variables.\",\n",
       " \"As we are talking about weekly sales, let's separate a month of sales for our Out of Time analysis.\",\n",
       " 'As we find that holidays have different weights in store sales, we will separate in variables to assist our analysis.',\n",
       " 'Overview',\n",
       " \"Let's remove the variables with some collinearity with Markdonw, as well as the categorical variables and the Weekly Sale Size that served only for a punctual analysis of behavior.\",\n",
       " \"We can see that Thanksgiving holidays has a higher sales value than other holidays. Let's consider this in the predictions. \\n We also make sure that type C stores have no strong influence on holidays.\",\n",
       " \"Stores C has a very different behavior, which can cause some problems in our model. Let's keep the type variables to try to reduce this problem.\",\n",
       " 'Linear regression',\n",
       " 'We analyze the variables and possible information for the model and raised some hypotheses and conclusions:\\n1) To obtain a low WMAE we consider departments as a high power variable for the model. For this we assume that the numbers of the departments have consistency between the stores, that is, the dept 1 of store 1 represents the same department as dept 1 of store 45. Another way to model would be to use the total sum of sales by stores and Break sales by department according to the representativeness of each department per store. We did the simulation and found interesting results, but a little distant from type C stores.\\n2) If you were to deal with a prediction for future stores, it would be interesting not to consider the store and department variable in predictions, in order to not know the results through known stores.\\n3) In case of future sales predictions, where we do not know the aggregate features, we could seek the approach of temporal series, and try the fit of an Arima model for the data, for this we should use the weekly sales differences added by store, Given that this variable Weekly Sales.diff () has a stationary behavior in TMEPO.',\n",
       " 'To compare the models that we will adjust forecasts using the function determined by the WMAE challenge',\n",
       " 'Applying Predictions',\n",
       " \"Three models were chosen to compare performance and check what would be the model adopted for this case: LinearRegression, randomphorrestor and gradientboostor. Each of the three models has a specificity for the data, so let's look at all. \\n Linear regression will serve us as a baseline for the performance of the models.\",\n",
       " \"Let's extract the columns of month and year to identify if holidays have different behavior in time\",\n",
       " \"Let's do a sales analysis by Walmart store and departments.\",\n",
       " 'Predictions',\n",
       " \"Let's separate the data sets and analyze the information that is contained in each\",\n",
       " 'Model',\n",
       " \"Let's use the crossvalidation method to see the coherence of our models as well as their stability in the data sets. \\n We will also use the gridsearch to see what are the optimal parts of each model we are using.\",\n",
       " \"One last analysis, we saw that sales of stores C do not behave the same as other stores, so let's see how they behave, dividing the store by its size.\",\n",
       " 'Layer stacking (5) \\n  Active function (relu) use optimization function SGD (existing bass line code) ADAM use \\n  Test data is also normalized',\n",
       " 'Sloan Digital Sky Survey',\n",
       " 'Divided into 2 train trains and tests',\n",
       " 'Install Captcha',\n",
       " 'Captcha creation function',\n",
       " 'Create episodes X, Y to train',\n",
       " 'Create captchas folder containing captcha created',\n",
       " 'Declare the necessary libraries',\n",
       " 'Create and Compile Model',\n",
       " 'Test model',\n",
       " 'Showing 1 random captcha',\n",
       " 'Train model',\n",
       " 'Declare the constants',\n",
       " 'From the above, it can be seen that the Title values also recognize gender very well',\n",
       " 'Only experienced: \\n\\n  People who had a title other than MR and the size of a family less than 5 and also \\n\\n  People who had a title other than MR and the size of a family greater than 4 and had a class not the lowest class',\n",
       " 'data=pd.read csv( kaggle input travel insurance travel insurance.csv )\\ndata.head()',\n",
       " 'Adding dollar data',\n",
       " '1.Model',\n",
       " 'Adding Moon and Year Data',\n",
       " \"Date of Month Expenditures (in the Format of Year and Moon) \\n\\n Type of Working Tour Expenditures (in advance or installment) \\n\\n Sector where sector expenditures are made \\n\\n Customer's unique customer number of each customer \\n\\n The number of customs made in the relevant date, species and sector \\n\\n The total amount of the transactions carried out by the customer in the relevant date, species and sector \\n\\n ID sector of the ID (sektor id in submission file)\",\n",
       " 'Ay',\n",
       " 'Year',\n",
       " 'Adding the Year Quarter Information',\n",
       " '5 Encoding',\n",
       " 'Customer Classification',\n",
       " '1. [Definition of Libraries] (1) \\n 1. [Reading data] (2) \\n 1. [Data Understanding] (3) \\n 1. [Feature Engineering] (4) \\n  1. [Transaction Type Replacement] (4.1) \\n  1. [Adding Moon and Year Data] (4.2) \\n  1. [Adding Dollar Data] (4.3) \\n  1. [Sector Classification] (4.4) \\n  1. [Adding Quarter Information] (4.5) \\n  1. [Customer Classification] (4.6) \\n  1. [average trading number and transaction amount of customers] (4.7) \\n  1. [Deleting unnecessary columns] (4.8) \\n 1. [Encoding] (5) \\n 1. [Train Test Split] (6) \\n 1. [Model] (7) \\n  1. [base model] (7.1) \\n  11th. Model] (7.2)',\n",
       " '4 Feature Engineering)',\n",
       " '2 Reading data',\n",
       " 'Process Type Change',\n",
       " '3 Understanding data',\n",
       " 'The average trading number and transaction amount of customers',\n",
       " '1 Definition of Libraries',\n",
       " 'QUARTER',\n",
       " 'Baz Model',\n",
       " 'Sector Class',\n",
       " 'Id',\n",
       " 'Class Class',\n",
       " '6 Model',\n",
       " 'Deletion of unnecessary columns',\n",
       " 'Sector classification',\n",
       " 'QUESTION 1 Item B Frequency Table \\n\\n Build a frequency table for each of the qualitative variables you have chosen (if you have not chosen, leave this question blank). A tip: Pandas Value Counts () function can be very useful. 🇧🇷',\n",
       " 'QUESTION 1 Item C Graphic Representation \\n For each of the variables, produce one or more graphs, using Matplotlib, which describes your characteristic behavior. Remember that these graphs need to be compatible with the variable classification.',\n",
       " \"Tips \\n\\n  Example of access to CSVs available on the Dataset \\n  df = pd.read CSV ('.. input anv.csv', delimiter = ',')\",\n",
       " 'Question 1 \\n  Statement: This notebook is associated with Kaggle Dataset called Exercise 1. This Kagle DataSet has two CSV format files (ANV.CSV and BR Electorate 2016 Municipality). Choose one of the available and already known datasets at your discretion. Once the CSV is defined, choose at least 7 and at most 12 variables (columns) that you evaluate as relevant. For each of its chosen variables, provide: \\n\\n\\n  Question 1 Item The classification of variables \\n\\n Classify all chosen variables, and build a dataframe with your answer. \\n Example:',\n",
       " 'DATAVIZ DATA SCIENCE SPECIALIZATION PROGRAM FACEN \\n\\n\\n  Exercise 1 First Contact with Kaggle \\n (worth grade) \\n\\n  Delivery Date: Until the end of the class \\n  Teacher: Matheus Mota \\n  Student: Eduardo Cesar Mourão \\n  RA: 203072',\n",
       " 'Create drawing class',\n",
       " 'Define the Googlenet model',\n",
       " 'training',\n",
       " 'Logistic Regression',\n",
       " '1 Import Libraries',\n",
       " 'Pie Chart Chart according to the reasons for being killed',\n",
       " 'Status of the murderers Pie Chart Chart',\n",
       " 'Determination of Numerical Values',\n",
       " \"Women's murders between January 2008 and August 2020 in Turkey \\n\\n\\n Contents: \\n\\n 1. [loading and controlling data] (1) \\n 2. [Data Description] (2) \\n 3. [Determination of Numerical Values] (3) \\n 4. [Visualization of Question and Data] (4) \\n  [Women's Histogram Graffus] (5) \\n  [Model of Killing Pie Chart Graph] (6) \\n  [Protection Decision Pie Chart Chart] (7) \\n  [Murderer's Status Pie Chart Graph] (8) \\n  [Who was killed by Bar Plot Graph] (9) \\n  [one. By those with a degree of blood ties] (10) \\n  [By people with emotional relationships] (11) \\n  [Pie Chart Graffic by Regions] (12) \\n  [Pie Chart Graf by cities] (13) \\n  [Pie Chart Graffic according to the reasons for being killed] (14) \\n 5. [Sources] (15) \\n 6. [Teammate] (16)\",\n",
       " 'Pie Chart Chart by Regions',\n",
       " 'Visualization of QUESTIONS AND DATA',\n",
       " 'Teammates \\n\\n  Yağmur Sezen Demir \\n  Büşra Durak',\n",
       " \"Definition of data \\n\\n EXPLANATION: \\n The data is taken from news sites on the internet and does not contain a hundred percent accuracy. Unfortunately, some data cannot be reached because it was not reported to the public afterwards. The data was created by Yağmur Sezen Demir and Büşra Durak with the withdrawal of the data on the internet through Selenium to Excel. \\n\\n  id: identifier \\n  City: Which city was killed \\n  ibid: Reşit is not reşit \\n  Date: on which date \\n  Protectionorder: Protection decision \\n  WHY1, WHY2: Reason for killing \\n  Killer1, Killer2: Who was killed by who \\n  Killingway1, Killingway2, Killingway3: The Machine's Milling \\n  Statusofiller: Killer's Status \\n  Year: in which year\",\n",
       " 'Pie Chart Chart by cities',\n",
       " 'The status of the killers is seen in the data above.',\n",
       " 'By those with 1st degree blood ties',\n",
       " 'Looking at the first 7 killing tools, it is seen that almost half are firearms.',\n",
       " 'In the question written above, those who were killed by a firearm between 2008 2020 were listed by her husband.',\n",
       " 'Bar Plot graphics',\n",
       " 'Protection decision Pie Chart Chart',\n",
       " ...]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_markdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate_chunks([val for val in nonenglish_markdowns if len(val) == 7547][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ff56cd",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba4c6c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_n = 9\n",
    "\n",
    "language_counts = train_final['markdown_language'].value_counts()\n",
    "\n",
    "top_n_languages = language_counts[:top_n]\n",
    "other_languages = language_counts[top_n:]\n",
    "\n",
    "language_counts_grouped = pd.DataFrame(top_n_languages).reset_index()\n",
    "language_counts_grouped.columns = ['markdown_language', 'counts']\n",
    "language_counts_grouped['markdown_language'] = language_counts_grouped['markdown_language'].apply(lambda x: x.upper())\n",
    "language_counts_grouped.loc[top_n+1] = ['Others', sum(other_languages.values)]\n",
    "\n",
    "\n",
    "fig = px.pie(language_counts_grouped, \n",
    "             values='counts', \n",
    "             names='markdown_language',\n",
    "             width=800, \n",
    "             height=1000,\n",
    "             title='Markdown Language Ratios')\n",
    "\n",
    "fig.update_traces(textinfo='percent',\n",
    "                  marker=dict(line=dict(color='white', width=3)))\n",
    "\n",
    "fig.update_traces()\n",
    "\n",
    "fig.show('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce1bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD WIDGETS TO SHOW NOTEBOOKS BEFORE AND AFTER TRANSLATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fef644",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_counts_codes_list = []\n",
    "line_counts_markdowns_list = []\n",
    "\n",
    "for r in range(train_final.shape[0]): \n",
    "    codes_line_list = ''.join(train_final['codes'][r]).split('\\n')\n",
    "    codes_line_list = [val for val in codes_line_list if val != '']\n",
    "    line_counts_codes = len(codes_line_list)\n",
    "    \n",
    "    markdowns_line_list = ''.join(train_final['markdowns_cleaned'][r]).split('\\n')\n",
    "    markdowns_line_list = [val for val in markdowns_line_list if val != '']\n",
    "    line_counts_markdowns = len(markdowns_line_list)\n",
    "    \n",
    "    line_counts_codes_list.append(line_counts_codes)\n",
    "    line_counts_markdowns_list.append(line_counts_markdowns)\n",
    "    \n",
    "line_counts_codes_list = np.array(line_counts_codes_list)    \n",
    "line_counts_markdowns_list = np.array(line_counts_markdowns_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e06824",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final['codes_line_counts'] = line_counts_codes_list\n",
    "train_final['markdowns_line_counts'] = line_counts_markdowns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accbde69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(y=line_counts_codes_list, \n",
    "                     name = 'Number of Lines (Codes)',\n",
    "                     marker_color = 'indianred'))\n",
    "\n",
    "fig.add_trace(go.Box(y=line_counts_markdowns_list,\n",
    "                     name = 'Number of Lines (Markdowns)',\n",
    "                     marker_color = 'lightseagreen'))\n",
    "\n",
    "fig.update_layout(template = 'plotly_white',\n",
    "                  width=1000,\n",
    "                  height=600,\n",
    "                  title=\"Distribution of the Number of Lines\")\n",
    "    \n",
    "\n",
    "fig.show('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d8309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "n_line_markdown_ratio = np.mean(line_counts_markdowns_list<=n)\n",
    "n_line_markdown_ratio = np.round(n_line_markdown_ratio, 2)\n",
    "\n",
    "print(\"The ratio of notebooks that have lower than or equal to {} line(s) in their markdowns: {}\".format(n, n_line_markdown_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1,26))\n",
    "y = [np.mean(line_counts_markdowns_list<=n) for n in x]\n",
    "\n",
    "fig = go.Figure([go.Bar(x=x, y=y)])\n",
    "\n",
    "fig.update_traces(marker_color='rgb(158,202,225)', \n",
    "                  marker_line_color='white',\n",
    "                  marker_line_width=0.50, opacity=1)\n",
    "\n",
    "fig.update_layout(template='plotly_white',\n",
    "                  width=1000,\n",
    "                  height=600,\n",
    "                  title=\"Cumulative Bar Chart\",\n",
    "                  xaxis_title=\"Number of Lines\",\n",
    "                  yaxis_title=\"Ratio of Notebooks\")\n",
    "\n",
    "fig.update_xaxes(showline=True, \n",
    "                 showgrid=True, \n",
    "                 gridwidth=1.5, \n",
    "                 linewidth=2, \n",
    "                 nticks = max(x)+1)\n",
    "\n",
    "fig.update_yaxes(showline=True, \n",
    "                 showgrid=True, \n",
    "                 gridwidth=1.5, \n",
    "                 linewidth=2, \n",
    "                 nticks=11,\n",
    "                 range = [0,1])\n",
    "\n",
    "fig.show('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575aec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_counts_codes_sorted = pd.Series(line_counts_codes_list).sort_values(ascending=False)\n",
    "long_codes_indices = line_counts_codes_sorted[line_counts_codes_sorted>100].index.tolist()\n",
    "\n",
    "line_counts_markdowns_sorted = pd.Series(line_counts_markdowns_list).sort_values(ascending=False)\n",
    "long_markdowns_indices = line_counts_markdowns_sorted[line_counts_markdowns_sorted>100].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4525c034",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "long_markdown = train_final['markdowns_cleaned'][long_markdowns_indices[398]][:3]\n",
    "long_markdown_joined = '\\n'.join(long_markdown)\n",
    "\n",
    "print(long_markdown_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e533d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = [ts.google(val) for val in texts]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6968a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonenglish_index = train_final['language']!='en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_final['markdowns_translated'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd10c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_final.loc[~nonenglish_index, 'markdowns_translated'] = train_final['markdowns_cleaned'][~nonenglish_index].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574755c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# tqdm.pandas()\n",
    "# train_final.loc[nonenglish_index, 'markdowns_translated'] = train_final['markdowns_cleaned'][nonenglish_index].progress_apply(translate).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a27b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for val in range(len(markdowns)):\n",
    "    print(markdowns[val])\n",
    "    print()\n",
    "    print('---------')\n",
    "    print()\n",
    "    print(translated_markdowns[val])\n",
    "    print()\n",
    "    print('#########')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13cd5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANCESTRY (?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
